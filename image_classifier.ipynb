{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\andyk\\\\OneDrive\\\\Documents\\\\GitHub\\\\yoga-pose-classification'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "poses = ['downdog', \"goddess\", \"mountain\", \"tree\", \"warrior1\", \"warrior2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "image_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeCorruptedImages(path):\n",
    "    for filename in os.listdir(path):\n",
    "        try:\n",
    "            print(filename)\n",
    "            img = Image.open(os.path.join(path,filename))\n",
    "            img.verify() \n",
    "        except (IOError, SyntaxError) as e:\n",
    "            print('Bad file:', filename)\n",
    "            os.remove(os.path.join(path,filename))\n",
    "\n",
    "def convert_to_jpg(path):\n",
    "    for file in os.listdir(path):\n",
    "        #print(path, file)\n",
    "        img = Image.open(os.path.join(path, file))\n",
    "        img = img.convert('RGB')\n",
    "        file_name, file_ext = os.path.splitext(file)\n",
    "        #print(file_name, file_ext)\n",
    "        if file_ext != '.jpg':\n",
    "            print(file_name)\n",
    "            print(file_ext)\n",
    "            img.save(os.path.join(path, f'{file_name}.jpg'))\n",
    "            #os.remove(os.path.join(path, file_name))\n",
    "\n",
    "        \n",
    "    for file in os.listdir(path):\n",
    "        if not file.endswith(\".jpg\"):\n",
    "            os.remove(os.path.join(path, file))\n",
    "\n",
    "base_path = \"./dataset/poses/train/\"\n",
    "\n",
    "# for pose in poses:\n",
    "#     path = os.path.join(base_path, pose)\n",
    "#     removeCorruptedImages(os.path.join(base_path, pose))\n",
    "#     convert_to_jpg(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_transform = transforms.Compose(\n",
    "[\n",
    "    transforms.RandomResizedCrop(size=image_size, scale=(0.8, 1.0)),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]\n",
    "    )\n",
    "\n",
    "test_transform = transforms.Compose(\n",
    "[   transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]\n",
    "    )\n",
    "\n",
    "train = torchvision.datasets.ImageFolder(root=\"dataset/poses/train/\",\n",
    "                                    transform=train_transform)\n",
    "train_loader = torch.utils.data.DataLoader(train,\n",
    "                                        batch_size=batch_size, shuffle=True,\n",
    "                                        num_workers=4)\n",
    "\n",
    "\n",
    "\n",
    "test = torchvision.datasets.ImageFolder(root=\"dataset/poses/test/\",\n",
    "                                    transform=test_transform)\n",
    "test_loader = torch.utils.data.DataLoader(test,\n",
    "                                        batch_size=batch_size, shuffle=True,\n",
    "                                        num_workers=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_size = len(train)\n",
    "#valid_data_size = len(test)\n",
    "test_data_size = len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_outputs = False\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=7, stride=4, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(12)\n",
    "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=12, kernel_size=7, stride=2, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(12)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.conv4 = nn.Conv2d(in_channels=12, out_channels=12, kernel_size=7, stride=2, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(12)\n",
    "        self.conv5 = nn.Conv2d(in_channels=12, out_channels=12, kernel_size=5, stride=2, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(12)\n",
    "        self.conv6 = nn.Conv2d(in_channels=12, out_channels=12, kernel_size=5, stride=1, padding=1)\n",
    "        self.bn6 = nn.BatchNorm2d(12)\n",
    "        self.fc1 = nn.Linear(12*len(poses)*len(poses), len(poses))\n",
    "\n",
    "    def forward(self, input):\n",
    "        if print_outputs:\n",
    "            print('FORWARD')\n",
    "            print(input.shape)\n",
    "        output = F.relu(self.bn1(self.conv1(input)))  \n",
    "        if print_outputs:\n",
    "            print(output.shape)    \n",
    "        output = F.relu(self.bn2(self.conv2(output)))  \n",
    "        if print_outputs:\n",
    "            print(output.shape)     \n",
    "        output = self.pool(output)     \n",
    "        if print_outputs:\n",
    "            print(output.shape)                      \n",
    "        output = F.relu(self.bn4(self.conv4(output)))     \n",
    "        if print_outputs:\n",
    "            print(output.shape)   \n",
    "        output = F.relu(self.bn5(self.conv5(output)))\n",
    "        if print_outputs:\n",
    "            print(output.shape)   \n",
    "        output = output.view(-1, 12*len(poses)*len(poses))\n",
    "        if print_outputs:\n",
    "            print(output.shape)   \n",
    "        output = self.fc1(output)\n",
    "        if print_outputs:\n",
    "            print(output.shape)   \n",
    "\n",
    "        return output\n",
    "\n",
    "# Instantiate a neural network model \n",
    "model = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "# Define the loss function with Classification Cross-Entropy loss and an optimizer with Adam optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "\n",
    "# Function to save the model\n",
    "def saveModel():\n",
    "    path = \"./myFirstModel.pth\"\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "# Function to test the model with the test dataset and print the accuracy for the test images\n",
    "def testAccuracy():\n",
    "    \n",
    "    model.eval()\n",
    "    accuracy = 0.0\n",
    "    total = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            # run the model on the test set to predict labels\n",
    "            outputs = model(images)\n",
    "            # the label with the highest energy will be our prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            accuracy += (predicted == labels).sum().item()\n",
    "    \n",
    "    # compute the accuracy over all test images\n",
    "    accuracy = (100 * accuracy / total)\n",
    "    return(accuracy)\n",
    "\n",
    "\n",
    "# Training function. We simply have to loop over our data iterator and feed the inputs to the network and optimize.\n",
    "def train(num_epochs):\n",
    "    \n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    # Define your execution device\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"The model will be running on\", device, \"device\")\n",
    "    # Convert model parameters and buffers to CPU or Cuda\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "\n",
    "        for i, (images, labels) in enumerate(train_loader, 0):\n",
    "            \n",
    "            # get the inputs\n",
    "            images = Variable(images.to(device))\n",
    "            labels = Variable(labels.to(device))\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # predict classes using images from the training set\n",
    "            outputs = model(images)\n",
    "            # print(\"outputs shape\")\n",
    "            # print(outputs.shape)\n",
    "            # compute the loss based on model output and real labels\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            # backpropagate the loss\n",
    "            loss.backward()\n",
    "            # adjust parameters based on the calculated gradients\n",
    "            optimizer.step()\n",
    "\n",
    "            # Let's print statistics for every 1,000 images\n",
    "            running_loss += loss.item()     # extract the loss value\n",
    "            if i%100 == 0:\n",
    "                # print every 1000 (twice per epoch) \n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 1000))\n",
    "                # zero the loss\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Compute and print the average accuracy fo this epoch when tested over all 10000 test images\n",
    "        accuracy = testAccuracy()\n",
    "        print('For epoch', epoch+1,'the test accuracy over the whole test set is %d %%' % (accuracy))\n",
    "        \n",
    "        # we want to save the model if the accuracy is the best\n",
    "        if accuracy > best_accuracy:\n",
    "            saveModel()\n",
    "            best_accuracy = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Function to show the images\n",
    "def imageshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Function to test the model with a batch of images and show the labels predictions\n",
    "def testBatch():\n",
    "    # get batch of images from the test DataLoader  \n",
    "    images, labels = next(iter(test_loader))\n",
    "\n",
    "    # show all images as one image grid\n",
    "    imageshow(torchvision.utils.make_grid(images))\n",
    "   \n",
    "    # Show the real labels on the screen \n",
    "    print('Real labels: ', ' '.join('%5s' % poses[labels[j]] \n",
    "                               for j in range(batch_size)))\n",
    "  \n",
    "    # Let's see what if the model identifiers the  labels of those example\n",
    "    outputs = model(images)\n",
    "    \n",
    "    # We got the probability for every 10 labels. The highest (max) probability should be correct label\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    \n",
    "    # Let's show the predicted labels on the screen to compare with the real ones\n",
    "    print('Predicted: ', ' '.join('%5s' % poses[predicted[j]] \n",
    "                              for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test what classes performed well\n",
    "def testClasses():\n",
    "    class_correct = list(0. for i in range(len(poses)))\n",
    "    class_total = list(0. for i in range(len(poses)))\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            c = (predicted == labels).squeeze()\n",
    "            for i in range(batch_size):\n",
    "                label = labels[i]\n",
    "                class_correct[label] += c[i].item()\n",
    "                class_total[label] += 1\n",
    "\n",
    "    for i in range(len(poses)):\n",
    "        print('Accuracy of %5s : %2d %%' % (\n",
    "            poses[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testBatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testClasses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build our model\n",
    "train(5)\n",
    "print('Finished Training')\n",
    "\n",
    "# # Test which classes performed well\n",
    "# testModelAccuracy()\n",
    "\n",
    "# Let's load the model we just created and test the accuracy per label\n",
    "model = Network()\n",
    "path = \"myFirstModel.pth\"\n",
    "model.load_state_dict(torch.load(path))\n",
    "\n",
    "# Test with batch of images\n",
    "testBatch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andyk\\OneDrive\\Documents\\GitHub\\yoga-pose-classification\\venv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\andyk\\OneDrive\\Documents\\GitHub\\yoga-pose-classification\\venv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "resnet18 = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze model parameters\n",
    "for param in resnet18.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the final layer of ResNet50 Model for Transfer Learning\n",
    "fc_inputs = resnet18.fc.in_features\n",
    "resnet18.fc = nn.Sequential(\n",
    "    nn.Linear(fc_inputs, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.4),\n",
    "    nn.Linear(256, len(poses)), \n",
    "    nn.LogSoftmax(dim=1) # For using NLLLoss()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Optimizer and Loss Function\n",
    "loss_func = nn.NLLLoss()\n",
    "optimizer = Adam(resnet18.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "resnet18 = resnet18.to(device)\n",
    "#device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10\n",
      "Batch number: 000, Training: Loss: 1.6277, Accuracy: 0.4375\n",
      "Batch number: 001, Training: Loss: 1.7234, Accuracy: 0.3125\n",
      "Batch number: 002, Training: Loss: 1.7017, Accuracy: 0.3125\n",
      "Batch number: 003, Training: Loss: 1.8370, Accuracy: 0.1250\n",
      "Batch number: 004, Training: Loss: 1.8073, Accuracy: 0.3125\n",
      "Batch number: 005, Training: Loss: 1.6252, Accuracy: 0.2500\n",
      "Batch number: 006, Training: Loss: 1.6676, Accuracy: 0.2500\n",
      "Batch number: 007, Training: Loss: 1.7731, Accuracy: 0.1250\n",
      "Batch number: 008, Training: Loss: 1.5799, Accuracy: 0.3125\n",
      "Batch number: 009, Training: Loss: 1.6770, Accuracy: 0.2500\n",
      "Batch number: 010, Training: Loss: 1.7865, Accuracy: 0.2500\n",
      "Batch number: 011, Training: Loss: 1.6375, Accuracy: 0.3125\n",
      "Batch number: 012, Training: Loss: 1.8075, Accuracy: 0.3125\n",
      "Batch number: 013, Training: Loss: 1.7647, Accuracy: 0.2500\n",
      "Batch number: 014, Training: Loss: 1.5530, Accuracy: 0.5000\n",
      "Batch number: 015, Training: Loss: 1.7876, Accuracy: 0.1875\n",
      "Batch number: 016, Training: Loss: 1.6832, Accuracy: 0.1875\n",
      "Batch number: 017, Training: Loss: 1.5806, Accuracy: 0.3125\n",
      "Batch number: 018, Training: Loss: 1.6979, Accuracy: 0.2500\n",
      "Batch number: 019, Training: Loss: 1.6796, Accuracy: 0.3750\n",
      "Batch number: 020, Training: Loss: 1.6859, Accuracy: 0.2500\n",
      "Batch number: 021, Training: Loss: 1.6943, Accuracy: 0.2500\n",
      "Batch number: 022, Training: Loss: 1.8523, Accuracy: 0.1875\n",
      "Batch number: 023, Training: Loss: 1.7230, Accuracy: 0.2500\n",
      "Batch number: 024, Training: Loss: 1.5201, Accuracy: 0.6875\n",
      "Batch number: 025, Training: Loss: 1.8032, Accuracy: 0.0625\n",
      "Batch number: 026, Training: Loss: 1.6079, Accuracy: 0.2500\n",
      "Batch number: 027, Training: Loss: 1.6561, Accuracy: 0.4375\n",
      "Batch number: 028, Training: Loss: 1.7223, Accuracy: 0.3125\n",
      "Batch number: 029, Training: Loss: 1.6989, Accuracy: 0.3125\n",
      "Batch number: 030, Training: Loss: 1.9523, Accuracy: 0.3750\n",
      "Batch number: 031, Training: Loss: 1.6746, Accuracy: 0.3750\n",
      "Batch number: 032, Training: Loss: 1.7296, Accuracy: 0.2500\n",
      "Batch number: 033, Training: Loss: 1.5190, Accuracy: 0.4375\n",
      "Batch number: 034, Training: Loss: 1.5168, Accuracy: 0.3750\n",
      "Batch number: 035, Training: Loss: 1.7274, Accuracy: 0.1875\n",
      "Batch number: 036, Training: Loss: 1.8269, Accuracy: 0.2500\n",
      "Batch number: 037, Training: Loss: 1.6314, Accuracy: 0.3125\n",
      "Batch number: 038, Training: Loss: 1.7152, Accuracy: 0.3750\n",
      "Batch number: 039, Training: Loss: 1.6954, Accuracy: 0.3125\n",
      "Batch number: 040, Training: Loss: 1.6909, Accuracy: 0.3125\n",
      "Batch number: 041, Training: Loss: 1.5554, Accuracy: 0.3750\n",
      "Batch number: 042, Training: Loss: 1.5855, Accuracy: 0.4375\n",
      "Batch number: 043, Training: Loss: 1.7330, Accuracy: 0.4375\n",
      "Batch number: 044, Training: Loss: 1.7652, Accuracy: 0.2500\n",
      "Batch number: 045, Training: Loss: 1.6486, Accuracy: 0.3750\n",
      "Batch number: 046, Training: Loss: 1.5251, Accuracy: 0.4375\n",
      "Batch number: 047, Training: Loss: 1.7949, Accuracy: 0.1875\n",
      "Batch number: 048, Training: Loss: 1.3878, Accuracy: 0.5625\n",
      "Batch number: 049, Training: Loss: 1.6909, Accuracy: 0.2500\n",
      "Batch number: 050, Training: Loss: 1.6623, Accuracy: 0.1875\n",
      "Batch number: 051, Training: Loss: 1.6761, Accuracy: 0.1875\n",
      "Batch number: 052, Training: Loss: 1.5256, Accuracy: 0.3750\n",
      "Batch number: 053, Training: Loss: 1.6189, Accuracy: 0.3750\n",
      "Batch number: 054, Training: Loss: 1.7803, Accuracy: 0.3125\n",
      "Batch number: 055, Training: Loss: 1.6213, Accuracy: 0.4375\n",
      "Batch number: 056, Training: Loss: 1.7017, Accuracy: 0.1875\n",
      "Batch number: 057, Training: Loss: 1.4906, Accuracy: 0.5625\n",
      "Batch number: 058, Training: Loss: 1.6682, Accuracy: 0.1875\n",
      "Batch number: 059, Training: Loss: 1.6946, Accuracy: 0.2500\n",
      "Batch number: 060, Training: Loss: 1.4996, Accuracy: 0.5625\n",
      "Batch number: 061, Training: Loss: 1.4570, Accuracy: 0.5000\n",
      "Batch number: 062, Training: Loss: 1.6998, Accuracy: 0.3125\n",
      "Batch number: 063, Training: Loss: 1.6067, Accuracy: 0.3125\n",
      "Batch number: 064, Training: Loss: 1.7350, Accuracy: 0.1875\n",
      "Batch number: 065, Training: Loss: 1.8317, Accuracy: 0.1875\n",
      "Batch number: 066, Training: Loss: 1.4395, Accuracy: 0.5000\n",
      "Batch number: 067, Training: Loss: 1.3865, Accuracy: 0.5000\n",
      "Batch number: 068, Training: Loss: 1.8272, Accuracy: 0.1250\n",
      "Batch number: 069, Training: Loss: 1.8045, Accuracy: 0.1250\n",
      "Batch number: 070, Training: Loss: 1.5079, Accuracy: 0.5000\n",
      "Batch number: 071, Training: Loss: 1.5790, Accuracy: 0.4375\n",
      "Batch number: 072, Training: Loss: 1.6889, Accuracy: 0.2500\n",
      "Batch number: 073, Training: Loss: 1.6344, Accuracy: 0.3125\n",
      "Batch number: 074, Training: Loss: 1.5110, Accuracy: 0.4375\n",
      "Batch number: 075, Training: Loss: 1.6866, Accuracy: 0.1875\n",
      "Batch number: 076, Training: Loss: 1.7103, Accuracy: 0.2500\n",
      "Batch number: 077, Training: Loss: 1.6696, Accuracy: 0.4375\n",
      "Batch number: 078, Training: Loss: 1.6320, Accuracy: 0.2500\n",
      "Batch number: 079, Training: Loss: 1.7425, Accuracy: 0.1875\n",
      "Batch number: 080, Training: Loss: 1.5402, Accuracy: 0.3750\n",
      "Batch number: 081, Training: Loss: 1.7070, Accuracy: 0.1875\n",
      "Batch number: 082, Training: Loss: 1.6946, Accuracy: 0.2500\n",
      "Batch number: 083, Training: Loss: 1.7046, Accuracy: 0.2500\n",
      "Batch number: 084, Training: Loss: 1.5869, Accuracy: 0.3750\n",
      "Batch number: 085, Training: Loss: 1.6167, Accuracy: 0.3750\n",
      "Batch number: 086, Training: Loss: 1.6691, Accuracy: 0.5000\n",
      "Batch number: 087, Training: Loss: 1.5130, Accuracy: 0.3125\n",
      "Batch number: 088, Training: Loss: 1.4867, Accuracy: 0.5000\n",
      "Batch number: 089, Training: Loss: 1.4281, Accuracy: 0.5000\n",
      "Batch number: 090, Training: Loss: 1.2648, Accuracy: 0.5625\n",
      "Batch number: 091, Training: Loss: 1.6457, Accuracy: 0.2500\n",
      "Batch number: 092, Training: Loss: 1.6558, Accuracy: 0.2500\n",
      "Batch number: 093, Training: Loss: 1.5545, Accuracy: 0.3125\n",
      "Batch number: 094, Training: Loss: 1.8149, Accuracy: 0.1250\n",
      "Batch number: 095, Training: Loss: 1.5770, Accuracy: 0.3125\n",
      "Batch number: 096, Training: Loss: 1.3348, Accuracy: 0.7500\n",
      "Batch number: 097, Training: Loss: 1.5496, Accuracy: 0.3750\n",
      "Batch number: 098, Training: Loss: 1.4146, Accuracy: 0.3750\n",
      "Batch number: 099, Training: Loss: 1.4271, Accuracy: 0.5000\n",
      "Batch number: 100, Training: Loss: 1.6764, Accuracy: 0.3750\n",
      "Batch number: 101, Training: Loss: 1.7468, Accuracy: 0.1875\n",
      "Batch number: 102, Training: Loss: 1.6432, Accuracy: 0.3125\n",
      "Batch number: 103, Training: Loss: 1.7413, Accuracy: 0.1875\n",
      "Batch number: 104, Training: Loss: 1.6646, Accuracy: 0.1250\n",
      "Batch number: 105, Training: Loss: 1.5947, Accuracy: 0.3125\n",
      "Batch number: 106, Training: Loss: 1.3912, Accuracy: 0.4375\n",
      "Batch number: 107, Training: Loss: 1.6854, Accuracy: 0.2500\n",
      "Batch number: 108, Training: Loss: 1.6194, Accuracy: 0.3125\n",
      "Batch number: 109, Training: Loss: 1.6452, Accuracy: 0.3125\n",
      "Batch number: 110, Training: Loss: 1.4990, Accuracy: 0.5000\n",
      "Batch number: 111, Training: Loss: 1.4124, Accuracy: 0.5000\n",
      "Batch number: 112, Training: Loss: 1.6630, Accuracy: 0.3750\n",
      "Batch number: 113, Training: Loss: 1.6420, Accuracy: 0.1875\n",
      "Batch number: 114, Training: Loss: 1.4774, Accuracy: 0.5000\n",
      "Batch number: 115, Training: Loss: 1.5408, Accuracy: 0.6250\n",
      "Batch number: 116, Training: Loss: 1.6003, Accuracy: 0.2500\n",
      "Batch number: 117, Training: Loss: 1.4763, Accuracy: 0.3750\n",
      "Batch number: 118, Training: Loss: 1.6665, Accuracy: 0.4375\n",
      "Batch number: 119, Training: Loss: 1.5690, Accuracy: 0.2500\n",
      "Batch number: 120, Training: Loss: 1.6061, Accuracy: 0.3125\n",
      "Batch number: 121, Training: Loss: 1.5470, Accuracy: 0.4375\n",
      "Batch number: 122, Training: Loss: 1.6718, Accuracy: 0.2500\n",
      "Batch number: 123, Training: Loss: 1.3881, Accuracy: 0.5000\n",
      "Batch number: 124, Training: Loss: 1.4505, Accuracy: 0.3750\n",
      "Batch number: 125, Training: Loss: 1.4928, Accuracy: 0.4375\n",
      "Batch number: 126, Training: Loss: 1.5202, Accuracy: 0.3750\n",
      "Batch number: 127, Training: Loss: 1.3944, Accuracy: 0.5625\n",
      "Batch number: 128, Training: Loss: 1.4874, Accuracy: 0.5000\n",
      "Batch number: 129, Training: Loss: 1.6291, Accuracy: 0.3750\n",
      "Batch number: 130, Training: Loss: 1.5382, Accuracy: 0.4375\n",
      "Batch number: 131, Training: Loss: 1.3085, Accuracy: 0.5000\n",
      "Batch number: 132, Training: Loss: 1.6509, Accuracy: 0.1875\n",
      "Batch number: 133, Training: Loss: 1.3269, Accuracy: 0.2500\n",
      "Batch number: 134, Training: Loss: 1.5925, Accuracy: 0.2500\n",
      "Batch number: 135, Training: Loss: 1.4939, Accuracy: 0.3125\n",
      "Batch number: 136, Training: Loss: 1.5243, Accuracy: 0.5000\n",
      "Batch number: 137, Training: Loss: 1.5171, Accuracy: 0.4375\n",
      "Batch number: 138, Training: Loss: 1.3373, Accuracy: 0.6250\n",
      "Batch number: 139, Training: Loss: 1.4585, Accuracy: 0.4375\n",
      "Batch number: 140, Training: Loss: 1.8131, Accuracy: 0.1875\n",
      "Batch number: 141, Training: Loss: 2.0122, Accuracy: 0.1250\n",
      "Batch number: 142, Training: Loss: 1.5465, Accuracy: 0.4375\n",
      "Batch number: 143, Training: Loss: 1.5825, Accuracy: 0.4375\n",
      "Batch number: 144, Training: Loss: 1.5140, Accuracy: 0.2500\n",
      "Batch number: 145, Training: Loss: 1.4083, Accuracy: 0.4375\n",
      "Batch number: 146, Training: Loss: 1.5710, Accuracy: 0.3750\n",
      "Batch number: 147, Training: Loss: 1.3515, Accuracy: 0.5625\n",
      "Batch number: 148, Training: Loss: 1.6329, Accuracy: 0.2500\n",
      "Batch number: 149, Training: Loss: 1.6742, Accuracy: 0.2500\n",
      "Batch number: 150, Training: Loss: 1.5088, Accuracy: 0.3125\n",
      "Batch number: 151, Training: Loss: 1.5806, Accuracy: 0.3750\n",
      "Batch number: 152, Training: Loss: 1.4637, Accuracy: 0.5625\n",
      "Batch number: 153, Training: Loss: 1.5262, Accuracy: 0.3750\n",
      "Batch number: 154, Training: Loss: 1.7300, Accuracy: 0.3333\n",
      "Epoch: 2/10\n",
      "Batch number: 000, Training: Loss: 1.7004, Accuracy: 0.3750\n",
      "Batch number: 001, Training: Loss: 1.6508, Accuracy: 0.3125\n",
      "Batch number: 002, Training: Loss: 1.7134, Accuracy: 0.2500\n",
      "Batch number: 003, Training: Loss: 1.6297, Accuracy: 0.3125\n",
      "Batch number: 004, Training: Loss: 1.4511, Accuracy: 0.5000\n",
      "Batch number: 005, Training: Loss: 1.5217, Accuracy: 0.4375\n",
      "Batch number: 006, Training: Loss: 1.5033, Accuracy: 0.3750\n",
      "Batch number: 007, Training: Loss: 1.7076, Accuracy: 0.1875\n",
      "Batch number: 008, Training: Loss: 1.3344, Accuracy: 0.5625\n",
      "Batch number: 009, Training: Loss: 1.2903, Accuracy: 0.6250\n",
      "Batch number: 010, Training: Loss: 1.5445, Accuracy: 0.5000\n",
      "Batch number: 011, Training: Loss: 1.3312, Accuracy: 0.4375\n",
      "Batch number: 012, Training: Loss: 1.5759, Accuracy: 0.4375\n",
      "Batch number: 013, Training: Loss: 1.2803, Accuracy: 0.7500\n",
      "Batch number: 014, Training: Loss: 1.3417, Accuracy: 0.5000\n",
      "Batch number: 015, Training: Loss: 1.4755, Accuracy: 0.3125\n",
      "Batch number: 016, Training: Loss: 1.4436, Accuracy: 0.3750\n",
      "Batch number: 017, Training: Loss: 1.5445, Accuracy: 0.1875\n",
      "Batch number: 018, Training: Loss: 1.6051, Accuracy: 0.1875\n",
      "Batch number: 019, Training: Loss: 1.5203, Accuracy: 0.3750\n",
      "Batch number: 020, Training: Loss: 1.4327, Accuracy: 0.5000\n",
      "Batch number: 021, Training: Loss: 1.5827, Accuracy: 0.3125\n",
      "Batch number: 022, Training: Loss: 1.4755, Accuracy: 0.4375\n",
      "Batch number: 023, Training: Loss: 1.6943, Accuracy: 0.3125\n",
      "Batch number: 024, Training: Loss: 1.7007, Accuracy: 0.3750\n",
      "Batch number: 025, Training: Loss: 1.3543, Accuracy: 0.6250\n",
      "Batch number: 026, Training: Loss: 1.3365, Accuracy: 0.5625\n",
      "Batch number: 027, Training: Loss: 1.3342, Accuracy: 0.5625\n",
      "Batch number: 028, Training: Loss: 1.3466, Accuracy: 0.4375\n",
      "Batch number: 029, Training: Loss: 1.3511, Accuracy: 0.5000\n",
      "Batch number: 030, Training: Loss: 1.5314, Accuracy: 0.3750\n",
      "Batch number: 031, Training: Loss: 1.4201, Accuracy: 0.4375\n",
      "Batch number: 032, Training: Loss: 1.3145, Accuracy: 0.6250\n",
      "Batch number: 033, Training: Loss: 1.2829, Accuracy: 0.5000\n",
      "Batch number: 034, Training: Loss: 1.4673, Accuracy: 0.3750\n",
      "Batch number: 035, Training: Loss: 1.4246, Accuracy: 0.4375\n",
      "Batch number: 036, Training: Loss: 1.6414, Accuracy: 0.2500\n",
      "Batch number: 037, Training: Loss: 1.3392, Accuracy: 0.5000\n",
      "Batch number: 038, Training: Loss: 1.2801, Accuracy: 0.4375\n",
      "Batch number: 039, Training: Loss: 1.3895, Accuracy: 0.4375\n",
      "Batch number: 040, Training: Loss: 1.5251, Accuracy: 0.4375\n",
      "Batch number: 041, Training: Loss: 1.2558, Accuracy: 0.6875\n",
      "Batch number: 042, Training: Loss: 1.7423, Accuracy: 0.1875\n",
      "Batch number: 043, Training: Loss: 1.2967, Accuracy: 0.6875\n",
      "Batch number: 044, Training: Loss: 1.3543, Accuracy: 0.3750\n",
      "Batch number: 045, Training: Loss: 1.4866, Accuracy: 0.4375\n",
      "Batch number: 046, Training: Loss: 1.1710, Accuracy: 0.6250\n",
      "Batch number: 047, Training: Loss: 1.7074, Accuracy: 0.1875\n",
      "Batch number: 048, Training: Loss: 1.4957, Accuracy: 0.1250\n",
      "Batch number: 049, Training: Loss: 1.3152, Accuracy: 0.4375\n",
      "Batch number: 050, Training: Loss: 1.2316, Accuracy: 0.5625\n",
      "Batch number: 051, Training: Loss: 1.4066, Accuracy: 0.3750\n",
      "Batch number: 052, Training: Loss: 1.3753, Accuracy: 0.5000\n",
      "Batch number: 053, Training: Loss: 1.3511, Accuracy: 0.3125\n",
      "Batch number: 054, Training: Loss: 1.4693, Accuracy: 0.3750\n",
      "Batch number: 055, Training: Loss: 1.2421, Accuracy: 0.5625\n",
      "Batch number: 056, Training: Loss: 1.3817, Accuracy: 0.5000\n",
      "Batch number: 057, Training: Loss: 1.5478, Accuracy: 0.4375\n",
      "Batch number: 058, Training: Loss: 1.2046, Accuracy: 0.4375\n",
      "Batch number: 059, Training: Loss: 1.5038, Accuracy: 0.5625\n",
      "Batch number: 060, Training: Loss: 1.5001, Accuracy: 0.3125\n",
      "Batch number: 061, Training: Loss: 1.2333, Accuracy: 0.5625\n",
      "Batch number: 062, Training: Loss: 1.4018, Accuracy: 0.3750\n",
      "Batch number: 063, Training: Loss: 1.6196, Accuracy: 0.1875\n",
      "Batch number: 064, Training: Loss: 1.2798, Accuracy: 0.4375\n",
      "Batch number: 065, Training: Loss: 1.3487, Accuracy: 0.2500\n",
      "Batch number: 066, Training: Loss: 1.5528, Accuracy: 0.4375\n",
      "Batch number: 067, Training: Loss: 1.2535, Accuracy: 0.6875\n",
      "Batch number: 068, Training: Loss: 1.2820, Accuracy: 0.5000\n",
      "Batch number: 069, Training: Loss: 1.2785, Accuracy: 0.5625\n",
      "Batch number: 070, Training: Loss: 1.3208, Accuracy: 0.4375\n",
      "Batch number: 071, Training: Loss: 1.7151, Accuracy: 0.3125\n",
      "Batch number: 072, Training: Loss: 1.2856, Accuracy: 0.4375\n",
      "Batch number: 073, Training: Loss: 1.4806, Accuracy: 0.5000\n",
      "Batch number: 074, Training: Loss: 1.4895, Accuracy: 0.2500\n",
      "Batch number: 075, Training: Loss: 1.9186, Accuracy: 0.1250\n",
      "Batch number: 076, Training: Loss: 1.4096, Accuracy: 0.4375\n",
      "Batch number: 077, Training: Loss: 1.5212, Accuracy: 0.3750\n",
      "Batch number: 078, Training: Loss: 1.3233, Accuracy: 0.6250\n",
      "Batch number: 079, Training: Loss: 1.1873, Accuracy: 0.4375\n",
      "Batch number: 080, Training: Loss: 1.3918, Accuracy: 0.3750\n",
      "Batch number: 081, Training: Loss: 1.4036, Accuracy: 0.4375\n",
      "Batch number: 082, Training: Loss: 1.3467, Accuracy: 0.5000\n",
      "Batch number: 083, Training: Loss: 1.2479, Accuracy: 0.5000\n",
      "Batch number: 084, Training: Loss: 1.6157, Accuracy: 0.0000\n",
      "Batch number: 085, Training: Loss: 1.2590, Accuracy: 0.6250\n",
      "Batch number: 086, Training: Loss: 1.3466, Accuracy: 0.5625\n",
      "Batch number: 087, Training: Loss: 1.5411, Accuracy: 0.5000\n",
      "Batch number: 088, Training: Loss: 1.3803, Accuracy: 0.3750\n",
      "Batch number: 089, Training: Loss: 1.3055, Accuracy: 0.5000\n",
      "Batch number: 090, Training: Loss: 1.4172, Accuracy: 0.3750\n",
      "Batch number: 091, Training: Loss: 1.5374, Accuracy: 0.4375\n",
      "Batch number: 092, Training: Loss: 1.4707, Accuracy: 0.5000\n",
      "Batch number: 093, Training: Loss: 1.5300, Accuracy: 0.3750\n",
      "Batch number: 094, Training: Loss: 1.3905, Accuracy: 0.3750\n",
      "Batch number: 095, Training: Loss: 1.2290, Accuracy: 0.6875\n",
      "Batch number: 096, Training: Loss: 1.4139, Accuracy: 0.3125\n",
      "Batch number: 097, Training: Loss: 1.6956, Accuracy: 0.0625\n",
      "Batch number: 098, Training: Loss: 1.4668, Accuracy: 0.3125\n",
      "Batch number: 099, Training: Loss: 1.5236, Accuracy: 0.5625\n",
      "Batch number: 100, Training: Loss: 1.4419, Accuracy: 0.4375\n",
      "Batch number: 101, Training: Loss: 1.2341, Accuracy: 0.5625\n",
      "Batch number: 102, Training: Loss: 1.1608, Accuracy: 0.6250\n",
      "Batch number: 103, Training: Loss: 1.3770, Accuracy: 0.4375\n",
      "Batch number: 104, Training: Loss: 1.2984, Accuracy: 0.5000\n",
      "Batch number: 105, Training: Loss: 1.2606, Accuracy: 0.5000\n",
      "Batch number: 106, Training: Loss: 1.2332, Accuracy: 0.5000\n",
      "Batch number: 107, Training: Loss: 1.5712, Accuracy: 0.3125\n",
      "Batch number: 108, Training: Loss: 1.5207, Accuracy: 0.2500\n",
      "Batch number: 109, Training: Loss: 1.2266, Accuracy: 0.5000\n",
      "Batch number: 110, Training: Loss: 1.3482, Accuracy: 0.3125\n",
      "Batch number: 111, Training: Loss: 1.1342, Accuracy: 0.5625\n",
      "Batch number: 112, Training: Loss: 1.1619, Accuracy: 0.6250\n",
      "Batch number: 113, Training: Loss: 1.5456, Accuracy: 0.3750\n",
      "Batch number: 114, Training: Loss: 1.6188, Accuracy: 0.4375\n",
      "Batch number: 115, Training: Loss: 1.3705, Accuracy: 0.5000\n",
      "Batch number: 116, Training: Loss: 1.6390, Accuracy: 0.4375\n",
      "Batch number: 117, Training: Loss: 1.2460, Accuracy: 0.3750\n",
      "Batch number: 118, Training: Loss: 1.8699, Accuracy: 0.3125\n",
      "Batch number: 119, Training: Loss: 1.1212, Accuracy: 0.6250\n",
      "Batch number: 120, Training: Loss: 1.5875, Accuracy: 0.5000\n",
      "Batch number: 121, Training: Loss: 1.2759, Accuracy: 0.6875\n",
      "Batch number: 122, Training: Loss: 1.4128, Accuracy: 0.5000\n",
      "Batch number: 123, Training: Loss: 1.3473, Accuracy: 0.4375\n",
      "Batch number: 124, Training: Loss: 1.3992, Accuracy: 0.3125\n",
      "Batch number: 125, Training: Loss: 1.5149, Accuracy: 0.2500\n",
      "Batch number: 126, Training: Loss: 1.5091, Accuracy: 0.2500\n",
      "Batch number: 127, Training: Loss: 1.3745, Accuracy: 0.5000\n",
      "Batch number: 128, Training: Loss: 1.1936, Accuracy: 0.3750\n",
      "Batch number: 129, Training: Loss: 1.3987, Accuracy: 0.5625\n",
      "Batch number: 130, Training: Loss: 1.5610, Accuracy: 0.3750\n",
      "Batch number: 131, Training: Loss: 1.2676, Accuracy: 0.5000\n",
      "Batch number: 132, Training: Loss: 1.3446, Accuracy: 0.3750\n",
      "Batch number: 133, Training: Loss: 1.3044, Accuracy: 0.5625\n",
      "Batch number: 134, Training: Loss: 1.1790, Accuracy: 0.5000\n",
      "Batch number: 135, Training: Loss: 1.3310, Accuracy: 0.4375\n",
      "Batch number: 136, Training: Loss: 1.3863, Accuracy: 0.5000\n",
      "Batch number: 137, Training: Loss: 1.6002, Accuracy: 0.5000\n",
      "Batch number: 138, Training: Loss: 1.3399, Accuracy: 0.5000\n",
      "Batch number: 139, Training: Loss: 1.6163, Accuracy: 0.1250\n",
      "Batch number: 140, Training: Loss: 1.4430, Accuracy: 0.3750\n",
      "Batch number: 141, Training: Loss: 1.1558, Accuracy: 0.6875\n",
      "Batch number: 142, Training: Loss: 1.1827, Accuracy: 0.5000\n",
      "Batch number: 143, Training: Loss: 1.2894, Accuracy: 0.3125\n",
      "Batch number: 144, Training: Loss: 1.2130, Accuracy: 0.6250\n",
      "Batch number: 145, Training: Loss: 1.3464, Accuracy: 0.3750\n",
      "Batch number: 146, Training: Loss: 1.4303, Accuracy: 0.5000\n",
      "Batch number: 147, Training: Loss: 1.5902, Accuracy: 0.5000\n",
      "Batch number: 148, Training: Loss: 1.4498, Accuracy: 0.5000\n",
      "Batch number: 149, Training: Loss: 1.5049, Accuracy: 0.3750\n",
      "Batch number: 150, Training: Loss: 1.1408, Accuracy: 0.5625\n",
      "Batch number: 151, Training: Loss: 1.4871, Accuracy: 0.4375\n",
      "Batch number: 152, Training: Loss: 1.3307, Accuracy: 0.4375\n",
      "Batch number: 153, Training: Loss: 1.3510, Accuracy: 0.4375\n",
      "Batch number: 154, Training: Loss: 1.4233, Accuracy: 0.6667\n",
      "Epoch: 3/10\n",
      "Batch number: 000, Training: Loss: 1.2128, Accuracy: 0.5625\n",
      "Batch number: 001, Training: Loss: 1.1874, Accuracy: 0.5625\n",
      "Batch number: 002, Training: Loss: 1.2939, Accuracy: 0.4375\n",
      "Batch number: 003, Training: Loss: 1.5171, Accuracy: 0.4375\n",
      "Batch number: 004, Training: Loss: 1.4867, Accuracy: 0.4375\n",
      "Batch number: 005, Training: Loss: 1.4194, Accuracy: 0.4375\n",
      "Batch number: 006, Training: Loss: 1.5760, Accuracy: 0.4375\n",
      "Batch number: 007, Training: Loss: 1.0352, Accuracy: 0.8125\n",
      "Batch number: 008, Training: Loss: 1.3057, Accuracy: 0.4375\n",
      "Batch number: 009, Training: Loss: 1.3686, Accuracy: 0.5625\n",
      "Batch number: 010, Training: Loss: 1.3588, Accuracy: 0.5000\n",
      "Batch number: 011, Training: Loss: 1.4644, Accuracy: 0.3750\n",
      "Batch number: 012, Training: Loss: 1.0207, Accuracy: 0.7500\n",
      "Batch number: 013, Training: Loss: 1.1188, Accuracy: 0.6875\n",
      "Batch number: 014, Training: Loss: 0.9806, Accuracy: 0.6875\n",
      "Batch number: 015, Training: Loss: 1.7257, Accuracy: 0.2500\n",
      "Batch number: 016, Training: Loss: 1.5204, Accuracy: 0.3750\n",
      "Batch number: 017, Training: Loss: 1.1344, Accuracy: 0.6250\n",
      "Batch number: 018, Training: Loss: 1.2101, Accuracy: 0.6250\n",
      "Batch number: 019, Training: Loss: 1.6486, Accuracy: 0.2500\n",
      "Batch number: 020, Training: Loss: 1.1394, Accuracy: 0.6875\n",
      "Batch number: 021, Training: Loss: 1.3487, Accuracy: 0.6250\n",
      "Batch number: 022, Training: Loss: 1.2170, Accuracy: 0.3750\n",
      "Batch number: 023, Training: Loss: 1.5116, Accuracy: 0.2500\n",
      "Batch number: 024, Training: Loss: 1.2906, Accuracy: 0.4375\n",
      "Batch number: 025, Training: Loss: 1.2317, Accuracy: 0.5000\n",
      "Batch number: 026, Training: Loss: 1.2203, Accuracy: 0.5625\n",
      "Batch number: 027, Training: Loss: 1.2671, Accuracy: 0.6875\n",
      "Batch number: 028, Training: Loss: 1.2170, Accuracy: 0.4375\n",
      "Batch number: 029, Training: Loss: 1.2687, Accuracy: 0.6875\n",
      "Batch number: 030, Training: Loss: 1.3221, Accuracy: 0.5625\n",
      "Batch number: 031, Training: Loss: 1.5068, Accuracy: 0.4375\n",
      "Batch number: 032, Training: Loss: 0.9744, Accuracy: 0.6875\n",
      "Batch number: 033, Training: Loss: 1.2533, Accuracy: 0.5625\n",
      "Batch number: 034, Training: Loss: 1.1788, Accuracy: 0.5000\n",
      "Batch number: 035, Training: Loss: 1.1847, Accuracy: 0.5625\n",
      "Batch number: 036, Training: Loss: 1.2775, Accuracy: 0.4375\n",
      "Batch number: 037, Training: Loss: 1.0930, Accuracy: 0.6875\n",
      "Batch number: 038, Training: Loss: 1.2187, Accuracy: 0.5000\n",
      "Batch number: 039, Training: Loss: 1.3730, Accuracy: 0.4375\n",
      "Batch number: 040, Training: Loss: 1.2589, Accuracy: 0.5625\n",
      "Batch number: 041, Training: Loss: 1.3466, Accuracy: 0.5625\n",
      "Batch number: 042, Training: Loss: 1.0680, Accuracy: 0.7500\n",
      "Batch number: 043, Training: Loss: 1.2385, Accuracy: 0.5000\n",
      "Batch number: 044, Training: Loss: 1.6313, Accuracy: 0.3125\n",
      "Batch number: 045, Training: Loss: 1.0946, Accuracy: 0.6875\n",
      "Batch number: 046, Training: Loss: 1.3705, Accuracy: 0.3125\n",
      "Batch number: 047, Training: Loss: 1.0767, Accuracy: 0.6250\n",
      "Batch number: 048, Training: Loss: 1.3942, Accuracy: 0.3125\n",
      "Batch number: 049, Training: Loss: 1.1096, Accuracy: 0.5000\n",
      "Batch number: 050, Training: Loss: 0.9983, Accuracy: 0.6875\n",
      "Batch number: 051, Training: Loss: 1.0721, Accuracy: 0.5625\n",
      "Batch number: 052, Training: Loss: 1.2381, Accuracy: 0.6250\n",
      "Batch number: 053, Training: Loss: 1.1246, Accuracy: 0.5000\n",
      "Batch number: 054, Training: Loss: 0.9618, Accuracy: 0.6250\n",
      "Batch number: 055, Training: Loss: 1.4233, Accuracy: 0.2500\n",
      "Batch number: 056, Training: Loss: 1.1415, Accuracy: 0.6250\n",
      "Batch number: 057, Training: Loss: 1.2553, Accuracy: 0.3750\n",
      "Batch number: 058, Training: Loss: 1.2921, Accuracy: 0.4375\n",
      "Batch number: 059, Training: Loss: 1.3512, Accuracy: 0.2500\n",
      "Batch number: 060, Training: Loss: 1.2568, Accuracy: 0.5625\n",
      "Batch number: 061, Training: Loss: 1.1758, Accuracy: 0.5625\n",
      "Batch number: 062, Training: Loss: 1.4163, Accuracy: 0.4375\n",
      "Batch number: 063, Training: Loss: 1.4036, Accuracy: 0.5000\n",
      "Batch number: 064, Training: Loss: 1.5798, Accuracy: 0.4375\n",
      "Batch number: 065, Training: Loss: 1.0940, Accuracy: 0.5625\n",
      "Batch number: 066, Training: Loss: 1.2444, Accuracy: 0.5000\n",
      "Batch number: 067, Training: Loss: 1.4280, Accuracy: 0.3125\n",
      "Batch number: 068, Training: Loss: 1.1547, Accuracy: 0.4375\n",
      "Batch number: 069, Training: Loss: 1.1152, Accuracy: 0.6250\n",
      "Batch number: 070, Training: Loss: 1.3010, Accuracy: 0.3750\n",
      "Batch number: 071, Training: Loss: 1.5953, Accuracy: 0.1875\n",
      "Batch number: 072, Training: Loss: 1.1269, Accuracy: 0.5000\n",
      "Batch number: 073, Training: Loss: 1.0228, Accuracy: 0.6250\n",
      "Batch number: 074, Training: Loss: 1.0484, Accuracy: 0.7500\n",
      "Batch number: 075, Training: Loss: 1.1612, Accuracy: 0.4375\n",
      "Batch number: 076, Training: Loss: 1.4773, Accuracy: 0.5625\n",
      "Batch number: 077, Training: Loss: 1.4633, Accuracy: 0.5000\n",
      "Batch number: 078, Training: Loss: 1.2968, Accuracy: 0.5625\n",
      "Batch number: 079, Training: Loss: 1.2425, Accuracy: 0.5000\n",
      "Batch number: 080, Training: Loss: 1.1450, Accuracy: 0.4375\n",
      "Batch number: 081, Training: Loss: 1.2738, Accuracy: 0.5625\n",
      "Batch number: 082, Training: Loss: 1.2454, Accuracy: 0.5625\n",
      "Batch number: 083, Training: Loss: 1.5169, Accuracy: 0.4375\n",
      "Batch number: 084, Training: Loss: 1.0961, Accuracy: 0.6875\n",
      "Batch number: 085, Training: Loss: 1.2592, Accuracy: 0.5625\n",
      "Batch number: 086, Training: Loss: 1.3521, Accuracy: 0.5625\n",
      "Batch number: 087, Training: Loss: 0.9157, Accuracy: 0.7500\n",
      "Batch number: 088, Training: Loss: 1.7257, Accuracy: 0.2500\n",
      "Batch number: 089, Training: Loss: 1.2004, Accuracy: 0.5625\n",
      "Batch number: 090, Training: Loss: 1.3908, Accuracy: 0.3750\n",
      "Batch number: 091, Training: Loss: 1.3760, Accuracy: 0.5625\n",
      "Batch number: 092, Training: Loss: 1.2542, Accuracy: 0.3750\n",
      "Batch number: 093, Training: Loss: 1.2100, Accuracy: 0.5000\n",
      "Batch number: 094, Training: Loss: 1.0095, Accuracy: 0.7500\n",
      "Batch number: 095, Training: Loss: 1.1878, Accuracy: 0.5625\n",
      "Batch number: 096, Training: Loss: 1.2502, Accuracy: 0.5000\n",
      "Batch number: 097, Training: Loss: 1.3461, Accuracy: 0.5000\n",
      "Batch number: 098, Training: Loss: 1.2827, Accuracy: 0.5000\n",
      "Batch number: 099, Training: Loss: 1.2616, Accuracy: 0.4375\n",
      "Batch number: 100, Training: Loss: 1.4589, Accuracy: 0.5625\n",
      "Batch number: 101, Training: Loss: 0.9694, Accuracy: 0.6875\n",
      "Batch number: 102, Training: Loss: 1.0962, Accuracy: 0.5000\n",
      "Batch number: 103, Training: Loss: 0.9450, Accuracy: 0.7500\n",
      "Batch number: 104, Training: Loss: 1.2271, Accuracy: 0.6250\n",
      "Batch number: 105, Training: Loss: 1.1921, Accuracy: 0.5000\n",
      "Batch number: 106, Training: Loss: 1.1857, Accuracy: 0.5000\n",
      "Batch number: 107, Training: Loss: 1.0070, Accuracy: 0.6250\n",
      "Batch number: 108, Training: Loss: 1.3958, Accuracy: 0.3750\n",
      "Batch number: 109, Training: Loss: 1.1509, Accuracy: 0.5000\n",
      "Batch number: 110, Training: Loss: 1.1971, Accuracy: 0.5625\n",
      "Batch number: 111, Training: Loss: 1.3644, Accuracy: 0.5000\n",
      "Batch number: 112, Training: Loss: 0.8998, Accuracy: 0.6250\n",
      "Batch number: 113, Training: Loss: 1.0523, Accuracy: 0.5625\n",
      "Batch number: 114, Training: Loss: 1.1416, Accuracy: 0.5625\n",
      "Batch number: 115, Training: Loss: 1.0141, Accuracy: 0.7500\n",
      "Batch number: 116, Training: Loss: 1.6818, Accuracy: 0.3125\n",
      "Batch number: 117, Training: Loss: 1.1330, Accuracy: 0.5000\n",
      "Batch number: 118, Training: Loss: 1.1343, Accuracy: 0.5625\n",
      "Batch number: 119, Training: Loss: 1.1462, Accuracy: 0.5625\n",
      "Batch number: 120, Training: Loss: 0.9886, Accuracy: 0.7500\n",
      "Batch number: 121, Training: Loss: 1.4789, Accuracy: 0.5625\n",
      "Batch number: 122, Training: Loss: 1.0172, Accuracy: 0.5625\n",
      "Batch number: 123, Training: Loss: 1.0700, Accuracy: 0.5000\n",
      "Batch number: 124, Training: Loss: 1.5514, Accuracy: 0.5000\n",
      "Batch number: 125, Training: Loss: 1.2486, Accuracy: 0.5000\n",
      "Batch number: 126, Training: Loss: 1.2027, Accuracy: 0.5000\n",
      "Batch number: 127, Training: Loss: 0.8969, Accuracy: 0.6875\n",
      "Batch number: 128, Training: Loss: 1.0968, Accuracy: 0.6250\n",
      "Batch number: 129, Training: Loss: 1.0414, Accuracy: 0.5000\n",
      "Batch number: 130, Training: Loss: 1.5001, Accuracy: 0.2500\n",
      "Batch number: 131, Training: Loss: 1.3138, Accuracy: 0.3750\n",
      "Batch number: 132, Training: Loss: 1.2463, Accuracy: 0.4375\n",
      "Batch number: 133, Training: Loss: 1.0825, Accuracy: 0.5625\n",
      "Batch number: 134, Training: Loss: 1.1977, Accuracy: 0.4375\n",
      "Batch number: 135, Training: Loss: 1.1158, Accuracy: 0.6250\n",
      "Batch number: 136, Training: Loss: 1.1390, Accuracy: 0.5000\n",
      "Batch number: 137, Training: Loss: 1.0926, Accuracy: 0.6250\n",
      "Batch number: 138, Training: Loss: 1.3950, Accuracy: 0.2500\n",
      "Batch number: 139, Training: Loss: 1.3417, Accuracy: 0.5000\n",
      "Batch number: 140, Training: Loss: 1.2313, Accuracy: 0.5625\n",
      "Batch number: 141, Training: Loss: 1.0882, Accuracy: 0.6250\n",
      "Batch number: 142, Training: Loss: 0.9413, Accuracy: 0.7500\n",
      "Batch number: 143, Training: Loss: 1.5764, Accuracy: 0.3750\n",
      "Batch number: 144, Training: Loss: 0.9402, Accuracy: 0.6875\n",
      "Batch number: 145, Training: Loss: 1.0671, Accuracy: 0.6875\n",
      "Batch number: 146, Training: Loss: 1.1145, Accuracy: 0.8125\n",
      "Batch number: 147, Training: Loss: 1.7175, Accuracy: 0.5000\n",
      "Batch number: 148, Training: Loss: 1.2411, Accuracy: 0.4375\n",
      "Batch number: 149, Training: Loss: 1.4585, Accuracy: 0.3750\n",
      "Batch number: 150, Training: Loss: 1.2803, Accuracy: 0.5625\n",
      "Batch number: 151, Training: Loss: 1.1036, Accuracy: 0.6250\n",
      "Batch number: 152, Training: Loss: 1.4208, Accuracy: 0.3125\n",
      "Batch number: 153, Training: Loss: 1.2657, Accuracy: 0.5000\n",
      "Batch number: 154, Training: Loss: 1.1485, Accuracy: 0.6667\n",
      "Epoch: 4/10\n",
      "Batch number: 000, Training: Loss: 1.2746, Accuracy: 0.5000\n",
      "Batch number: 001, Training: Loss: 1.2647, Accuracy: 0.4375\n",
      "Batch number: 002, Training: Loss: 0.9918, Accuracy: 0.8125\n",
      "Batch number: 003, Training: Loss: 1.3224, Accuracy: 0.5000\n",
      "Batch number: 004, Training: Loss: 1.0386, Accuracy: 0.6875\n",
      "Batch number: 005, Training: Loss: 1.3445, Accuracy: 0.3750\n",
      "Batch number: 006, Training: Loss: 1.3588, Accuracy: 0.5000\n",
      "Batch number: 007, Training: Loss: 0.9342, Accuracy: 0.6250\n",
      "Batch number: 008, Training: Loss: 0.9505, Accuracy: 0.8750\n",
      "Batch number: 009, Training: Loss: 1.6331, Accuracy: 0.3750\n",
      "Batch number: 010, Training: Loss: 1.0642, Accuracy: 0.5000\n",
      "Batch number: 011, Training: Loss: 0.9826, Accuracy: 0.6875\n",
      "Batch number: 012, Training: Loss: 1.1136, Accuracy: 0.5625\n",
      "Batch number: 013, Training: Loss: 0.8203, Accuracy: 0.7500\n",
      "Batch number: 014, Training: Loss: 1.1778, Accuracy: 0.5625\n",
      "Batch number: 015, Training: Loss: 1.0724, Accuracy: 0.6250\n",
      "Batch number: 016, Training: Loss: 1.0265, Accuracy: 0.7500\n",
      "Batch number: 017, Training: Loss: 1.0726, Accuracy: 0.5625\n",
      "Batch number: 018, Training: Loss: 1.0072, Accuracy: 0.7500\n",
      "Batch number: 019, Training: Loss: 0.9367, Accuracy: 0.6875\n",
      "Batch number: 020, Training: Loss: 1.2784, Accuracy: 0.6250\n",
      "Batch number: 021, Training: Loss: 1.4260, Accuracy: 0.5625\n",
      "Batch number: 022, Training: Loss: 1.1325, Accuracy: 0.5000\n",
      "Batch number: 023, Training: Loss: 1.2892, Accuracy: 0.5000\n",
      "Batch number: 024, Training: Loss: 0.9824, Accuracy: 0.5625\n",
      "Batch number: 025, Training: Loss: 1.1979, Accuracy: 0.6250\n",
      "Batch number: 026, Training: Loss: 1.2962, Accuracy: 0.5000\n",
      "Batch number: 027, Training: Loss: 0.9828, Accuracy: 0.6875\n",
      "Batch number: 028, Training: Loss: 1.0895, Accuracy: 0.5625\n",
      "Batch number: 029, Training: Loss: 0.9454, Accuracy: 0.6875\n",
      "Batch number: 030, Training: Loss: 0.8248, Accuracy: 0.8125\n",
      "Batch number: 031, Training: Loss: 0.8854, Accuracy: 0.6875\n",
      "Batch number: 032, Training: Loss: 1.5046, Accuracy: 0.4375\n",
      "Batch number: 033, Training: Loss: 0.9813, Accuracy: 0.6250\n",
      "Batch number: 034, Training: Loss: 1.2365, Accuracy: 0.5625\n",
      "Batch number: 035, Training: Loss: 1.1698, Accuracy: 0.6875\n",
      "Batch number: 036, Training: Loss: 0.9177, Accuracy: 0.6875\n",
      "Batch number: 037, Training: Loss: 1.1689, Accuracy: 0.4375\n",
      "Batch number: 038, Training: Loss: 1.5820, Accuracy: 0.3125\n",
      "Batch number: 039, Training: Loss: 0.8045, Accuracy: 0.6875\n",
      "Batch number: 040, Training: Loss: 0.9540, Accuracy: 0.6250\n",
      "Batch number: 041, Training: Loss: 0.8844, Accuracy: 0.6875\n",
      "Batch number: 042, Training: Loss: 1.1250, Accuracy: 0.3750\n",
      "Batch number: 043, Training: Loss: 1.2404, Accuracy: 0.3125\n",
      "Batch number: 044, Training: Loss: 0.9031, Accuracy: 0.6875\n",
      "Batch number: 045, Training: Loss: 1.1278, Accuracy: 0.5000\n",
      "Batch number: 046, Training: Loss: 1.6038, Accuracy: 0.3125\n",
      "Batch number: 047, Training: Loss: 1.8894, Accuracy: 0.3125\n",
      "Batch number: 048, Training: Loss: 1.1400, Accuracy: 0.6250\n",
      "Batch number: 049, Training: Loss: 1.3559, Accuracy: 0.5000\n",
      "Batch number: 050, Training: Loss: 0.9014, Accuracy: 0.7500\n",
      "Batch number: 051, Training: Loss: 1.0759, Accuracy: 0.7500\n",
      "Batch number: 052, Training: Loss: 0.9460, Accuracy: 0.6250\n",
      "Batch number: 053, Training: Loss: 0.8397, Accuracy: 0.6875\n",
      "Batch number: 054, Training: Loss: 1.2895, Accuracy: 0.3750\n",
      "Batch number: 055, Training: Loss: 1.3780, Accuracy: 0.3750\n",
      "Batch number: 056, Training: Loss: 0.9018, Accuracy: 0.8125\n",
      "Batch number: 057, Training: Loss: 1.5678, Accuracy: 0.4375\n",
      "Batch number: 058, Training: Loss: 1.0911, Accuracy: 0.6250\n",
      "Batch number: 059, Training: Loss: 1.3520, Accuracy: 0.5625\n",
      "Batch number: 060, Training: Loss: 1.4203, Accuracy: 0.5625\n",
      "Batch number: 061, Training: Loss: 1.2581, Accuracy: 0.4375\n",
      "Batch number: 062, Training: Loss: 1.1315, Accuracy: 0.5000\n",
      "Batch number: 063, Training: Loss: 1.0777, Accuracy: 0.5625\n",
      "Batch number: 064, Training: Loss: 1.2475, Accuracy: 0.5625\n",
      "Batch number: 065, Training: Loss: 1.1999, Accuracy: 0.5625\n",
      "Batch number: 066, Training: Loss: 1.3350, Accuracy: 0.5000\n",
      "Batch number: 067, Training: Loss: 1.3425, Accuracy: 0.3750\n",
      "Batch number: 068, Training: Loss: 1.3785, Accuracy: 0.3750\n",
      "Batch number: 069, Training: Loss: 1.3542, Accuracy: 0.5000\n",
      "Batch number: 070, Training: Loss: 0.8913, Accuracy: 0.6875\n",
      "Batch number: 071, Training: Loss: 0.8269, Accuracy: 0.8125\n",
      "Batch number: 072, Training: Loss: 1.2434, Accuracy: 0.6875\n",
      "Batch number: 073, Training: Loss: 1.3826, Accuracy: 0.5000\n",
      "Batch number: 074, Training: Loss: 0.7989, Accuracy: 0.7500\n",
      "Batch number: 075, Training: Loss: 1.1041, Accuracy: 0.5625\n",
      "Batch number: 076, Training: Loss: 1.5244, Accuracy: 0.4375\n",
      "Batch number: 077, Training: Loss: 1.2240, Accuracy: 0.5625\n",
      "Batch number: 078, Training: Loss: 1.3694, Accuracy: 0.3750\n",
      "Batch number: 079, Training: Loss: 1.1411, Accuracy: 0.7500\n",
      "Batch number: 080, Training: Loss: 1.3034, Accuracy: 0.4375\n",
      "Batch number: 081, Training: Loss: 1.1289, Accuracy: 0.5625\n",
      "Batch number: 082, Training: Loss: 1.3028, Accuracy: 0.4375\n",
      "Batch number: 083, Training: Loss: 1.1268, Accuracy: 0.5625\n",
      "Batch number: 084, Training: Loss: 1.0321, Accuracy: 0.6250\n",
      "Batch number: 085, Training: Loss: 0.8583, Accuracy: 0.8125\n",
      "Batch number: 086, Training: Loss: 1.3145, Accuracy: 0.4375\n",
      "Batch number: 087, Training: Loss: 1.4520, Accuracy: 0.3750\n",
      "Batch number: 088, Training: Loss: 1.5090, Accuracy: 0.2500\n",
      "Batch number: 089, Training: Loss: 1.1163, Accuracy: 0.6875\n",
      "Batch number: 090, Training: Loss: 0.9343, Accuracy: 0.7500\n",
      "Batch number: 091, Training: Loss: 1.1015, Accuracy: 0.5000\n",
      "Batch number: 092, Training: Loss: 1.3813, Accuracy: 0.5000\n",
      "Batch number: 093, Training: Loss: 1.0542, Accuracy: 0.5625\n",
      "Batch number: 094, Training: Loss: 1.2598, Accuracy: 0.5000\n",
      "Batch number: 095, Training: Loss: 1.3695, Accuracy: 0.5000\n",
      "Batch number: 096, Training: Loss: 1.4347, Accuracy: 0.4375\n",
      "Batch number: 097, Training: Loss: 1.0334, Accuracy: 0.6875\n",
      "Batch number: 098, Training: Loss: 1.1337, Accuracy: 0.6250\n",
      "Batch number: 099, Training: Loss: 1.3351, Accuracy: 0.3125\n",
      "Batch number: 100, Training: Loss: 1.2862, Accuracy: 0.5625\n",
      "Batch number: 101, Training: Loss: 1.4723, Accuracy: 0.3750\n",
      "Batch number: 102, Training: Loss: 1.0116, Accuracy: 0.6875\n",
      "Batch number: 103, Training: Loss: 0.9384, Accuracy: 0.5625\n",
      "Batch number: 104, Training: Loss: 1.1947, Accuracy: 0.6250\n",
      "Batch number: 105, Training: Loss: 1.1870, Accuracy: 0.6250\n",
      "Batch number: 106, Training: Loss: 1.2810, Accuracy: 0.5625\n",
      "Batch number: 107, Training: Loss: 1.0515, Accuracy: 0.4375\n",
      "Batch number: 108, Training: Loss: 1.1616, Accuracy: 0.4375\n",
      "Batch number: 109, Training: Loss: 1.1718, Accuracy: 0.6875\n",
      "Batch number: 110, Training: Loss: 1.0453, Accuracy: 0.5000\n",
      "Batch number: 111, Training: Loss: 1.2366, Accuracy: 0.4375\n",
      "Batch number: 112, Training: Loss: 1.3939, Accuracy: 0.4375\n",
      "Batch number: 113, Training: Loss: 1.0194, Accuracy: 0.5625\n",
      "Batch number: 114, Training: Loss: 1.2204, Accuracy: 0.5625\n",
      "Batch number: 115, Training: Loss: 1.0409, Accuracy: 0.5625\n",
      "Batch number: 116, Training: Loss: 1.4743, Accuracy: 0.5000\n",
      "Batch number: 117, Training: Loss: 0.9883, Accuracy: 0.7500\n",
      "Batch number: 118, Training: Loss: 1.1268, Accuracy: 0.6250\n",
      "Batch number: 119, Training: Loss: 1.2468, Accuracy: 0.5000\n",
      "Batch number: 120, Training: Loss: 1.3498, Accuracy: 0.4375\n",
      "Batch number: 121, Training: Loss: 1.0948, Accuracy: 0.5625\n",
      "Batch number: 122, Training: Loss: 0.8924, Accuracy: 0.6875\n",
      "Batch number: 123, Training: Loss: 0.8858, Accuracy: 0.7500\n",
      "Batch number: 124, Training: Loss: 1.2086, Accuracy: 0.4375\n",
      "Batch number: 125, Training: Loss: 0.9028, Accuracy: 0.6875\n",
      "Batch number: 126, Training: Loss: 1.1956, Accuracy: 0.5000\n",
      "Batch number: 127, Training: Loss: 1.0265, Accuracy: 0.5625\n",
      "Batch number: 128, Training: Loss: 1.3039, Accuracy: 0.3750\n",
      "Batch number: 129, Training: Loss: 1.0017, Accuracy: 0.5000\n",
      "Batch number: 130, Training: Loss: 1.5275, Accuracy: 0.5000\n",
      "Batch number: 131, Training: Loss: 0.9261, Accuracy: 0.6250\n",
      "Batch number: 132, Training: Loss: 0.9809, Accuracy: 0.5000\n",
      "Batch number: 133, Training: Loss: 0.8418, Accuracy: 0.6875\n",
      "Batch number: 134, Training: Loss: 1.2613, Accuracy: 0.5625\n",
      "Batch number: 135, Training: Loss: 1.3048, Accuracy: 0.3750\n",
      "Batch number: 136, Training: Loss: 1.1731, Accuracy: 0.5000\n",
      "Batch number: 137, Training: Loss: 0.9333, Accuracy: 0.7500\n",
      "Batch number: 138, Training: Loss: 1.3644, Accuracy: 0.4375\n",
      "Batch number: 139, Training: Loss: 0.7727, Accuracy: 0.8125\n",
      "Batch number: 140, Training: Loss: 1.1617, Accuracy: 0.5000\n",
      "Batch number: 141, Training: Loss: 1.3215, Accuracy: 0.4375\n",
      "Batch number: 142, Training: Loss: 1.0640, Accuracy: 0.6875\n",
      "Batch number: 143, Training: Loss: 1.4495, Accuracy: 0.1875\n",
      "Batch number: 144, Training: Loss: 0.7169, Accuracy: 0.8125\n",
      "Batch number: 145, Training: Loss: 1.4854, Accuracy: 0.6250\n",
      "Batch number: 146, Training: Loss: 1.0028, Accuracy: 0.5625\n",
      "Batch number: 147, Training: Loss: 0.9997, Accuracy: 0.5000\n",
      "Batch number: 148, Training: Loss: 1.1165, Accuracy: 0.5625\n",
      "Batch number: 149, Training: Loss: 1.2037, Accuracy: 0.5625\n",
      "Batch number: 150, Training: Loss: 1.3363, Accuracy: 0.5625\n",
      "Batch number: 151, Training: Loss: 1.3259, Accuracy: 0.3750\n",
      "Batch number: 152, Training: Loss: 1.1654, Accuracy: 0.5000\n",
      "Batch number: 153, Training: Loss: 0.9810, Accuracy: 0.6875\n",
      "Batch number: 154, Training: Loss: 1.4062, Accuracy: 0.3333\n",
      "Epoch: 5/10\n",
      "Batch number: 000, Training: Loss: 0.9835, Accuracy: 0.6250\n",
      "Batch number: 001, Training: Loss: 1.2120, Accuracy: 0.5625\n",
      "Batch number: 002, Training: Loss: 0.7624, Accuracy: 0.8125\n",
      "Batch number: 003, Training: Loss: 1.4497, Accuracy: 0.5625\n",
      "Batch number: 004, Training: Loss: 1.2464, Accuracy: 0.5625\n",
      "Batch number: 005, Training: Loss: 1.3367, Accuracy: 0.3750\n",
      "Batch number: 006, Training: Loss: 1.4071, Accuracy: 0.3750\n",
      "Batch number: 007, Training: Loss: 0.9412, Accuracy: 0.6875\n",
      "Batch number: 008, Training: Loss: 0.9332, Accuracy: 0.6875\n",
      "Batch number: 009, Training: Loss: 0.9113, Accuracy: 0.5625\n",
      "Batch number: 010, Training: Loss: 1.3384, Accuracy: 0.5625\n",
      "Batch number: 011, Training: Loss: 1.0787, Accuracy: 0.6250\n",
      "Batch number: 012, Training: Loss: 0.9902, Accuracy: 0.5000\n",
      "Batch number: 013, Training: Loss: 0.9341, Accuracy: 0.5000\n",
      "Batch number: 014, Training: Loss: 1.0778, Accuracy: 0.6250\n",
      "Batch number: 015, Training: Loss: 1.2849, Accuracy: 0.5000\n",
      "Batch number: 016, Training: Loss: 1.0274, Accuracy: 0.7500\n",
      "Batch number: 017, Training: Loss: 1.1025, Accuracy: 0.5625\n",
      "Batch number: 018, Training: Loss: 1.0419, Accuracy: 0.6875\n",
      "Batch number: 019, Training: Loss: 0.9307, Accuracy: 0.6250\n",
      "Batch number: 020, Training: Loss: 1.4381, Accuracy: 0.3750\n",
      "Batch number: 021, Training: Loss: 1.4047, Accuracy: 0.2500\n",
      "Batch number: 022, Training: Loss: 1.2068, Accuracy: 0.5625\n",
      "Batch number: 023, Training: Loss: 0.8561, Accuracy: 0.6875\n",
      "Batch number: 024, Training: Loss: 1.4304, Accuracy: 0.5000\n",
      "Batch number: 025, Training: Loss: 0.6901, Accuracy: 0.8125\n",
      "Batch number: 026, Training: Loss: 0.9717, Accuracy: 0.6875\n",
      "Batch number: 027, Training: Loss: 0.8893, Accuracy: 0.6875\n",
      "Batch number: 028, Training: Loss: 1.4878, Accuracy: 0.5000\n",
      "Batch number: 029, Training: Loss: 0.8599, Accuracy: 0.7500\n",
      "Batch number: 030, Training: Loss: 1.2438, Accuracy: 0.5000\n",
      "Batch number: 031, Training: Loss: 1.0268, Accuracy: 0.3750\n",
      "Batch number: 032, Training: Loss: 1.2795, Accuracy: 0.4375\n",
      "Batch number: 033, Training: Loss: 0.8894, Accuracy: 0.7500\n",
      "Batch number: 034, Training: Loss: 1.0736, Accuracy: 0.6250\n",
      "Batch number: 035, Training: Loss: 0.9282, Accuracy: 0.7500\n",
      "Batch number: 036, Training: Loss: 0.8654, Accuracy: 0.6875\n",
      "Batch number: 037, Training: Loss: 1.1018, Accuracy: 0.6875\n",
      "Batch number: 038, Training: Loss: 0.9631, Accuracy: 0.6250\n",
      "Batch number: 039, Training: Loss: 1.4921, Accuracy: 0.4375\n",
      "Batch number: 040, Training: Loss: 0.8876, Accuracy: 0.7500\n",
      "Batch number: 041, Training: Loss: 1.1165, Accuracy: 0.6875\n",
      "Batch number: 042, Training: Loss: 1.0335, Accuracy: 0.7500\n",
      "Batch number: 043, Training: Loss: 1.1826, Accuracy: 0.6875\n",
      "Batch number: 044, Training: Loss: 0.9908, Accuracy: 0.7500\n",
      "Batch number: 045, Training: Loss: 1.0789, Accuracy: 0.5000\n",
      "Batch number: 046, Training: Loss: 0.9601, Accuracy: 0.6250\n",
      "Batch number: 047, Training: Loss: 0.8157, Accuracy: 0.6250\n",
      "Batch number: 048, Training: Loss: 1.0531, Accuracy: 0.5000\n",
      "Batch number: 049, Training: Loss: 1.1508, Accuracy: 0.5000\n",
      "Batch number: 050, Training: Loss: 0.7060, Accuracy: 0.9375\n",
      "Batch number: 051, Training: Loss: 1.0000, Accuracy: 0.5625\n",
      "Batch number: 052, Training: Loss: 1.5136, Accuracy: 0.3750\n",
      "Batch number: 053, Training: Loss: 1.1501, Accuracy: 0.5625\n",
      "Batch number: 054, Training: Loss: 0.8292, Accuracy: 0.6250\n",
      "Batch number: 055, Training: Loss: 0.6989, Accuracy: 0.8125\n",
      "Batch number: 056, Training: Loss: 1.0575, Accuracy: 0.5625\n",
      "Batch number: 057, Training: Loss: 0.8283, Accuracy: 0.7500\n",
      "Batch number: 058, Training: Loss: 0.9272, Accuracy: 0.5000\n",
      "Batch number: 059, Training: Loss: 1.1640, Accuracy: 0.5625\n",
      "Batch number: 060, Training: Loss: 1.0724, Accuracy: 0.5000\n",
      "Batch number: 061, Training: Loss: 0.9163, Accuracy: 0.5625\n",
      "Batch number: 062, Training: Loss: 0.6444, Accuracy: 0.8125\n",
      "Batch number: 063, Training: Loss: 0.5223, Accuracy: 0.8750\n",
      "Batch number: 064, Training: Loss: 1.0454, Accuracy: 0.5000\n",
      "Batch number: 065, Training: Loss: 1.2385, Accuracy: 0.5000\n",
      "Batch number: 066, Training: Loss: 1.2799, Accuracy: 0.4375\n",
      "Batch number: 067, Training: Loss: 0.5759, Accuracy: 0.8125\n",
      "Batch number: 068, Training: Loss: 0.6851, Accuracy: 0.8750\n",
      "Batch number: 069, Training: Loss: 1.1086, Accuracy: 0.6250\n",
      "Batch number: 070, Training: Loss: 0.8921, Accuracy: 0.6875\n",
      "Batch number: 071, Training: Loss: 1.2752, Accuracy: 0.5000\n",
      "Batch number: 072, Training: Loss: 1.1582, Accuracy: 0.5625\n",
      "Batch number: 073, Training: Loss: 0.7322, Accuracy: 0.7500\n",
      "Batch number: 074, Training: Loss: 0.9219, Accuracy: 0.7500\n",
      "Batch number: 075, Training: Loss: 1.0086, Accuracy: 0.6875\n",
      "Batch number: 076, Training: Loss: 1.1766, Accuracy: 0.3750\n",
      "Batch number: 077, Training: Loss: 1.6172, Accuracy: 0.3125\n",
      "Batch number: 078, Training: Loss: 0.5992, Accuracy: 0.8750\n",
      "Batch number: 079, Training: Loss: 0.8994, Accuracy: 0.8125\n",
      "Batch number: 080, Training: Loss: 1.0299, Accuracy: 0.5625\n",
      "Batch number: 081, Training: Loss: 1.4119, Accuracy: 0.5000\n",
      "Batch number: 082, Training: Loss: 1.1393, Accuracy: 0.5000\n",
      "Batch number: 083, Training: Loss: 0.9254, Accuracy: 0.5625\n",
      "Batch number: 084, Training: Loss: 1.0131, Accuracy: 0.5625\n",
      "Batch number: 085, Training: Loss: 1.1035, Accuracy: 0.5625\n",
      "Batch number: 086, Training: Loss: 1.1000, Accuracy: 0.5625\n",
      "Batch number: 087, Training: Loss: 1.3534, Accuracy: 0.5000\n",
      "Batch number: 088, Training: Loss: 1.0306, Accuracy: 0.6250\n",
      "Batch number: 089, Training: Loss: 1.0301, Accuracy: 0.6250\n",
      "Batch number: 090, Training: Loss: 1.2837, Accuracy: 0.5000\n",
      "Batch number: 091, Training: Loss: 0.9546, Accuracy: 0.6250\n",
      "Batch number: 092, Training: Loss: 1.6555, Accuracy: 0.4375\n",
      "Batch number: 093, Training: Loss: 1.3742, Accuracy: 0.5000\n",
      "Batch number: 094, Training: Loss: 1.1163, Accuracy: 0.6250\n",
      "Batch number: 095, Training: Loss: 1.0589, Accuracy: 0.5000\n",
      "Batch number: 096, Training: Loss: 1.2662, Accuracy: 0.4375\n",
      "Batch number: 097, Training: Loss: 1.3078, Accuracy: 0.3750\n",
      "Batch number: 098, Training: Loss: 1.0660, Accuracy: 0.5625\n",
      "Batch number: 099, Training: Loss: 1.2898, Accuracy: 0.4375\n",
      "Batch number: 100, Training: Loss: 0.8717, Accuracy: 0.6875\n",
      "Batch number: 101, Training: Loss: 0.9157, Accuracy: 0.5625\n",
      "Batch number: 102, Training: Loss: 0.7719, Accuracy: 0.8125\n",
      "Batch number: 103, Training: Loss: 0.7396, Accuracy: 0.8125\n",
      "Batch number: 104, Training: Loss: 0.8534, Accuracy: 0.6875\n",
      "Batch number: 105, Training: Loss: 1.0791, Accuracy: 0.5000\n",
      "Batch number: 106, Training: Loss: 1.5540, Accuracy: 0.4375\n",
      "Batch number: 107, Training: Loss: 1.1519, Accuracy: 0.5625\n",
      "Batch number: 108, Training: Loss: 0.7452, Accuracy: 0.6875\n",
      "Batch number: 109, Training: Loss: 1.3346, Accuracy: 0.6250\n",
      "Batch number: 110, Training: Loss: 1.1013, Accuracy: 0.5000\n",
      "Batch number: 111, Training: Loss: 1.1152, Accuracy: 0.7500\n",
      "Batch number: 112, Training: Loss: 0.9528, Accuracy: 0.6250\n",
      "Batch number: 113, Training: Loss: 0.9281, Accuracy: 0.5000\n",
      "Batch number: 114, Training: Loss: 1.2555, Accuracy: 0.3750\n",
      "Batch number: 115, Training: Loss: 1.6240, Accuracy: 0.5625\n",
      "Batch number: 116, Training: Loss: 1.3708, Accuracy: 0.5000\n",
      "Batch number: 117, Training: Loss: 1.0141, Accuracy: 0.5625\n",
      "Batch number: 118, Training: Loss: 0.9076, Accuracy: 0.5625\n",
      "Batch number: 119, Training: Loss: 1.5522, Accuracy: 0.5000\n",
      "Batch number: 120, Training: Loss: 1.0729, Accuracy: 0.6250\n",
      "Batch number: 121, Training: Loss: 0.8837, Accuracy: 0.5625\n",
      "Batch number: 122, Training: Loss: 1.0386, Accuracy: 0.6875\n",
      "Batch number: 123, Training: Loss: 0.9520, Accuracy: 0.6250\n",
      "Batch number: 124, Training: Loss: 1.1451, Accuracy: 0.5625\n",
      "Batch number: 125, Training: Loss: 1.3459, Accuracy: 0.4375\n",
      "Batch number: 126, Training: Loss: 0.9024, Accuracy: 0.7500\n",
      "Batch number: 127, Training: Loss: 1.4680, Accuracy: 0.3750\n",
      "Batch number: 128, Training: Loss: 0.9171, Accuracy: 0.6875\n",
      "Batch number: 129, Training: Loss: 0.7591, Accuracy: 0.6250\n",
      "Batch number: 130, Training: Loss: 1.0899, Accuracy: 0.5625\n",
      "Batch number: 131, Training: Loss: 1.4298, Accuracy: 0.5000\n",
      "Batch number: 132, Training: Loss: 1.1476, Accuracy: 0.5000\n",
      "Batch number: 133, Training: Loss: 1.0080, Accuracy: 0.6250\n",
      "Batch number: 134, Training: Loss: 1.0192, Accuracy: 0.5625\n",
      "Batch number: 135, Training: Loss: 1.0820, Accuracy: 0.6875\n",
      "Batch number: 136, Training: Loss: 1.2024, Accuracy: 0.4375\n",
      "Batch number: 137, Training: Loss: 0.9199, Accuracy: 0.7500\n",
      "Batch number: 138, Training: Loss: 1.3642, Accuracy: 0.3125\n",
      "Batch number: 139, Training: Loss: 1.4137, Accuracy: 0.4375\n",
      "Batch number: 140, Training: Loss: 1.2919, Accuracy: 0.4375\n",
      "Batch number: 141, Training: Loss: 0.7413, Accuracy: 0.7500\n",
      "Batch number: 142, Training: Loss: 1.2821, Accuracy: 0.6250\n",
      "Batch number: 143, Training: Loss: 1.0829, Accuracy: 0.6875\n",
      "Batch number: 144, Training: Loss: 0.9168, Accuracy: 0.6250\n",
      "Batch number: 145, Training: Loss: 0.8266, Accuracy: 0.7500\n",
      "Batch number: 146, Training: Loss: 0.9881, Accuracy: 0.6875\n",
      "Batch number: 147, Training: Loss: 1.0528, Accuracy: 0.4375\n",
      "Batch number: 148, Training: Loss: 1.3652, Accuracy: 0.4375\n",
      "Batch number: 149, Training: Loss: 1.6452, Accuracy: 0.5625\n",
      "Batch number: 150, Training: Loss: 0.7729, Accuracy: 0.6250\n",
      "Batch number: 151, Training: Loss: 0.8971, Accuracy: 0.8125\n",
      "Batch number: 152, Training: Loss: 1.3076, Accuracy: 0.5000\n",
      "Batch number: 153, Training: Loss: 0.8963, Accuracy: 0.7500\n",
      "Batch number: 154, Training: Loss: 1.0333, Accuracy: 0.6667\n",
      "Epoch: 6/10\n",
      "Batch number: 000, Training: Loss: 0.6399, Accuracy: 0.8750\n",
      "Batch number: 001, Training: Loss: 0.8696, Accuracy: 0.7500\n",
      "Batch number: 002, Training: Loss: 1.1960, Accuracy: 0.3125\n",
      "Batch number: 003, Training: Loss: 0.9747, Accuracy: 0.6875\n",
      "Batch number: 004, Training: Loss: 0.9997, Accuracy: 0.5625\n",
      "Batch number: 005, Training: Loss: 0.8665, Accuracy: 0.6875\n",
      "Batch number: 006, Training: Loss: 1.0904, Accuracy: 0.3750\n",
      "Batch number: 007, Training: Loss: 1.2609, Accuracy: 0.7500\n",
      "Batch number: 008, Training: Loss: 1.0193, Accuracy: 0.6250\n",
      "Batch number: 009, Training: Loss: 1.1452, Accuracy: 0.6875\n",
      "Batch number: 010, Training: Loss: 1.0140, Accuracy: 0.5000\n",
      "Batch number: 011, Training: Loss: 0.9220, Accuracy: 0.6250\n",
      "Batch number: 012, Training: Loss: 0.9904, Accuracy: 0.6875\n",
      "Batch number: 013, Training: Loss: 1.1230, Accuracy: 0.5625\n",
      "Batch number: 014, Training: Loss: 1.0509, Accuracy: 0.6250\n",
      "Batch number: 015, Training: Loss: 1.0323, Accuracy: 0.6250\n",
      "Batch number: 016, Training: Loss: 0.9803, Accuracy: 0.6250\n",
      "Batch number: 017, Training: Loss: 1.1626, Accuracy: 0.5000\n",
      "Batch number: 018, Training: Loss: 1.1659, Accuracy: 0.5000\n",
      "Batch number: 019, Training: Loss: 0.8875, Accuracy: 0.6875\n",
      "Batch number: 020, Training: Loss: 1.4135, Accuracy: 0.5000\n",
      "Batch number: 021, Training: Loss: 0.7565, Accuracy: 0.6875\n",
      "Batch number: 022, Training: Loss: 0.6935, Accuracy: 0.8125\n",
      "Batch number: 023, Training: Loss: 0.9687, Accuracy: 0.6250\n",
      "Batch number: 024, Training: Loss: 0.8853, Accuracy: 0.6875\n",
      "Batch number: 025, Training: Loss: 1.1062, Accuracy: 0.6875\n",
      "Batch number: 026, Training: Loss: 0.9175, Accuracy: 0.6250\n",
      "Batch number: 027, Training: Loss: 0.8796, Accuracy: 0.6875\n",
      "Batch number: 028, Training: Loss: 0.8698, Accuracy: 0.6250\n",
      "Batch number: 029, Training: Loss: 0.7382, Accuracy: 0.7500\n",
      "Batch number: 030, Training: Loss: 1.0912, Accuracy: 0.6250\n",
      "Batch number: 031, Training: Loss: 1.2002, Accuracy: 0.5000\n",
      "Batch number: 032, Training: Loss: 1.2679, Accuracy: 0.3125\n",
      "Batch number: 033, Training: Loss: 1.2772, Accuracy: 0.5000\n",
      "Batch number: 034, Training: Loss: 0.8024, Accuracy: 0.6875\n",
      "Batch number: 035, Training: Loss: 0.9695, Accuracy: 0.5625\n",
      "Batch number: 036, Training: Loss: 0.8737, Accuracy: 0.6250\n",
      "Batch number: 037, Training: Loss: 1.1445, Accuracy: 0.6250\n",
      "Batch number: 038, Training: Loss: 1.1426, Accuracy: 0.6875\n",
      "Batch number: 039, Training: Loss: 0.9679, Accuracy: 0.6250\n",
      "Batch number: 040, Training: Loss: 1.2641, Accuracy: 0.6875\n",
      "Batch number: 041, Training: Loss: 0.8585, Accuracy: 0.7500\n",
      "Batch number: 042, Training: Loss: 0.7208, Accuracy: 0.8750\n",
      "Batch number: 043, Training: Loss: 0.7869, Accuracy: 0.8125\n",
      "Batch number: 044, Training: Loss: 1.0319, Accuracy: 0.7500\n",
      "Batch number: 045, Training: Loss: 1.3971, Accuracy: 0.5625\n",
      "Batch number: 046, Training: Loss: 0.9750, Accuracy: 0.5625\n",
      "Batch number: 047, Training: Loss: 1.0271, Accuracy: 0.6250\n",
      "Batch number: 048, Training: Loss: 1.0076, Accuracy: 0.5625\n",
      "Batch number: 049, Training: Loss: 0.9882, Accuracy: 0.6875\n",
      "Batch number: 050, Training: Loss: 0.8690, Accuracy: 0.6875\n",
      "Batch number: 051, Training: Loss: 0.9796, Accuracy: 0.5625\n",
      "Batch number: 052, Training: Loss: 1.3002, Accuracy: 0.3125\n",
      "Batch number: 053, Training: Loss: 1.2945, Accuracy: 0.4375\n",
      "Batch number: 054, Training: Loss: 0.9797, Accuracy: 0.6250\n",
      "Batch number: 055, Training: Loss: 1.2312, Accuracy: 0.5000\n",
      "Batch number: 056, Training: Loss: 1.0992, Accuracy: 0.5625\n",
      "Batch number: 057, Training: Loss: 1.0618, Accuracy: 0.5000\n",
      "Batch number: 058, Training: Loss: 1.2675, Accuracy: 0.5625\n",
      "Batch number: 059, Training: Loss: 0.8675, Accuracy: 0.6250\n",
      "Batch number: 060, Training: Loss: 0.6771, Accuracy: 0.8125\n",
      "Batch number: 061, Training: Loss: 1.0577, Accuracy: 0.5000\n",
      "Batch number: 062, Training: Loss: 1.1579, Accuracy: 0.5625\n",
      "Batch number: 063, Training: Loss: 1.6464, Accuracy: 0.3125\n",
      "Batch number: 064, Training: Loss: 1.3078, Accuracy: 0.3125\n",
      "Batch number: 065, Training: Loss: 1.0784, Accuracy: 0.5625\n",
      "Batch number: 066, Training: Loss: 0.8661, Accuracy: 0.7500\n",
      "Batch number: 067, Training: Loss: 0.9411, Accuracy: 0.6250\n",
      "Batch number: 068, Training: Loss: 1.2705, Accuracy: 0.5000\n",
      "Batch number: 069, Training: Loss: 1.3951, Accuracy: 0.3125\n",
      "Batch number: 070, Training: Loss: 0.8429, Accuracy: 0.7500\n",
      "Batch number: 071, Training: Loss: 0.8411, Accuracy: 0.7500\n",
      "Batch number: 072, Training: Loss: 0.8770, Accuracy: 0.7500\n",
      "Batch number: 073, Training: Loss: 0.7972, Accuracy: 0.7500\n",
      "Batch number: 074, Training: Loss: 0.9784, Accuracy: 0.5625\n",
      "Batch number: 075, Training: Loss: 0.9641, Accuracy: 0.6875\n",
      "Batch number: 076, Training: Loss: 0.9507, Accuracy: 0.6875\n",
      "Batch number: 077, Training: Loss: 0.9232, Accuracy: 0.6875\n",
      "Batch number: 078, Training: Loss: 1.3570, Accuracy: 0.5625\n",
      "Batch number: 079, Training: Loss: 0.7874, Accuracy: 0.6875\n",
      "Batch number: 080, Training: Loss: 0.8086, Accuracy: 0.7500\n",
      "Batch number: 081, Training: Loss: 0.8995, Accuracy: 0.6875\n",
      "Batch number: 082, Training: Loss: 0.5465, Accuracy: 0.6875\n",
      "Batch number: 083, Training: Loss: 1.0505, Accuracy: 0.6875\n",
      "Batch number: 084, Training: Loss: 1.0269, Accuracy: 0.7500\n",
      "Batch number: 085, Training: Loss: 1.0915, Accuracy: 0.5000\n",
      "Batch number: 086, Training: Loss: 0.7828, Accuracy: 0.6875\n",
      "Batch number: 087, Training: Loss: 0.8507, Accuracy: 0.5625\n",
      "Batch number: 088, Training: Loss: 0.9337, Accuracy: 0.5625\n",
      "Batch number: 089, Training: Loss: 1.3426, Accuracy: 0.3750\n",
      "Batch number: 090, Training: Loss: 1.1858, Accuracy: 0.3750\n",
      "Batch number: 091, Training: Loss: 0.7542, Accuracy: 0.7500\n",
      "Batch number: 092, Training: Loss: 0.7242, Accuracy: 0.8125\n",
      "Batch number: 093, Training: Loss: 1.1449, Accuracy: 0.6250\n",
      "Batch number: 094, Training: Loss: 0.8611, Accuracy: 0.6875\n",
      "Batch number: 095, Training: Loss: 1.0284, Accuracy: 0.6250\n",
      "Batch number: 096, Training: Loss: 0.8989, Accuracy: 0.6875\n",
      "Batch number: 097, Training: Loss: 1.3243, Accuracy: 0.3750\n",
      "Batch number: 098, Training: Loss: 0.9742, Accuracy: 0.6875\n",
      "Batch number: 099, Training: Loss: 1.3421, Accuracy: 0.6250\n",
      "Batch number: 100, Training: Loss: 1.1089, Accuracy: 0.6875\n",
      "Batch number: 101, Training: Loss: 1.2773, Accuracy: 0.5625\n",
      "Batch number: 102, Training: Loss: 1.4049, Accuracy: 0.4375\n",
      "Batch number: 103, Training: Loss: 0.8699, Accuracy: 0.6875\n",
      "Batch number: 104, Training: Loss: 0.8105, Accuracy: 0.6875\n",
      "Batch number: 105, Training: Loss: 0.8558, Accuracy: 0.6250\n",
      "Batch number: 106, Training: Loss: 1.0493, Accuracy: 0.6875\n",
      "Batch number: 107, Training: Loss: 0.7767, Accuracy: 0.5625\n",
      "Batch number: 108, Training: Loss: 1.0273, Accuracy: 0.5000\n",
      "Batch number: 109, Training: Loss: 0.8358, Accuracy: 0.5625\n",
      "Batch number: 110, Training: Loss: 1.1195, Accuracy: 0.5000\n",
      "Batch number: 111, Training: Loss: 1.3336, Accuracy: 0.5000\n",
      "Batch number: 112, Training: Loss: 1.1759, Accuracy: 0.6250\n",
      "Batch number: 113, Training: Loss: 1.1303, Accuracy: 0.6875\n",
      "Batch number: 114, Training: Loss: 0.7546, Accuracy: 0.8125\n",
      "Batch number: 115, Training: Loss: 0.8296, Accuracy: 0.6875\n",
      "Batch number: 116, Training: Loss: 0.9457, Accuracy: 0.6250\n",
      "Batch number: 117, Training: Loss: 0.7289, Accuracy: 0.7500\n",
      "Batch number: 118, Training: Loss: 0.7714, Accuracy: 0.7500\n",
      "Batch number: 119, Training: Loss: 0.9854, Accuracy: 0.6250\n",
      "Batch number: 120, Training: Loss: 1.3742, Accuracy: 0.4375\n",
      "Batch number: 121, Training: Loss: 0.7569, Accuracy: 0.6875\n",
      "Batch number: 122, Training: Loss: 1.0699, Accuracy: 0.6250\n",
      "Batch number: 123, Training: Loss: 1.0195, Accuracy: 0.7500\n",
      "Batch number: 124, Training: Loss: 1.3372, Accuracy: 0.3750\n",
      "Batch number: 125, Training: Loss: 1.2337, Accuracy: 0.6250\n",
      "Batch number: 126, Training: Loss: 0.9792, Accuracy: 0.5625\n",
      "Batch number: 127, Training: Loss: 1.2145, Accuracy: 0.5625\n",
      "Batch number: 128, Training: Loss: 0.8980, Accuracy: 0.7500\n",
      "Batch number: 129, Training: Loss: 0.9627, Accuracy: 0.6875\n",
      "Batch number: 130, Training: Loss: 1.2693, Accuracy: 0.5625\n",
      "Batch number: 131, Training: Loss: 1.9698, Accuracy: 0.5625\n",
      "Batch number: 132, Training: Loss: 0.7571, Accuracy: 0.7500\n",
      "Batch number: 133, Training: Loss: 0.9516, Accuracy: 0.5625\n",
      "Batch number: 134, Training: Loss: 1.4567, Accuracy: 0.5000\n",
      "Batch number: 135, Training: Loss: 1.0273, Accuracy: 0.6250\n",
      "Batch number: 136, Training: Loss: 1.5166, Accuracy: 0.3125\n",
      "Batch number: 137, Training: Loss: 1.1629, Accuracy: 0.5000\n",
      "Batch number: 138, Training: Loss: 0.9737, Accuracy: 0.6250\n",
      "Batch number: 139, Training: Loss: 0.8669, Accuracy: 0.6250\n",
      "Batch number: 140, Training: Loss: 1.1266, Accuracy: 0.5000\n",
      "Batch number: 141, Training: Loss: 1.0012, Accuracy: 0.6250\n",
      "Batch number: 142, Training: Loss: 0.8908, Accuracy: 0.8125\n",
      "Batch number: 143, Training: Loss: 0.9724, Accuracy: 0.6250\n",
      "Batch number: 144, Training: Loss: 1.2530, Accuracy: 0.7500\n",
      "Batch number: 145, Training: Loss: 0.9549, Accuracy: 0.5000\n",
      "Batch number: 146, Training: Loss: 1.3171, Accuracy: 0.4375\n",
      "Batch number: 147, Training: Loss: 1.0572, Accuracy: 0.5625\n",
      "Batch number: 148, Training: Loss: 0.9272, Accuracy: 0.7500\n",
      "Batch number: 149, Training: Loss: 0.8590, Accuracy: 0.6250\n",
      "Batch number: 150, Training: Loss: 1.0473, Accuracy: 0.6250\n",
      "Batch number: 151, Training: Loss: 1.3451, Accuracy: 0.5000\n",
      "Batch number: 152, Training: Loss: 1.2173, Accuracy: 0.4375\n",
      "Batch number: 153, Training: Loss: 1.0353, Accuracy: 0.7500\n",
      "Batch number: 154, Training: Loss: 0.8520, Accuracy: 1.0000\n",
      "Epoch: 7/10\n",
      "Batch number: 000, Training: Loss: 1.4140, Accuracy: 0.3125\n",
      "Batch number: 001, Training: Loss: 1.4218, Accuracy: 0.3125\n",
      "Batch number: 002, Training: Loss: 0.9795, Accuracy: 0.7500\n",
      "Batch number: 003, Training: Loss: 1.0480, Accuracy: 0.6875\n",
      "Batch number: 004, Training: Loss: 1.0312, Accuracy: 0.6250\n",
      "Batch number: 005, Training: Loss: 1.2245, Accuracy: 0.5000\n",
      "Batch number: 006, Training: Loss: 1.2341, Accuracy: 0.5000\n",
      "Batch number: 007, Training: Loss: 0.4975, Accuracy: 0.8125\n",
      "Batch number: 008, Training: Loss: 1.5066, Accuracy: 0.3750\n",
      "Batch number: 009, Training: Loss: 1.1093, Accuracy: 0.6250\n",
      "Batch number: 010, Training: Loss: 1.0172, Accuracy: 0.7500\n",
      "Batch number: 011, Training: Loss: 0.8884, Accuracy: 0.7500\n",
      "Batch number: 012, Training: Loss: 0.9313, Accuracy: 0.6875\n",
      "Batch number: 013, Training: Loss: 0.9979, Accuracy: 0.6875\n",
      "Batch number: 014, Training: Loss: 0.9246, Accuracy: 0.6875\n",
      "Batch number: 015, Training: Loss: 1.2483, Accuracy: 0.5625\n",
      "Batch number: 016, Training: Loss: 0.9039, Accuracy: 0.6250\n",
      "Batch number: 017, Training: Loss: 1.5626, Accuracy: 0.2500\n",
      "Batch number: 018, Training: Loss: 0.8694, Accuracy: 0.9375\n",
      "Batch number: 019, Training: Loss: 1.5897, Accuracy: 0.4375\n",
      "Batch number: 020, Training: Loss: 1.0744, Accuracy: 0.6250\n",
      "Batch number: 021, Training: Loss: 0.7969, Accuracy: 0.6875\n",
      "Batch number: 022, Training: Loss: 1.2513, Accuracy: 0.5625\n",
      "Batch number: 023, Training: Loss: 0.7709, Accuracy: 0.7500\n",
      "Batch number: 024, Training: Loss: 0.7742, Accuracy: 0.6875\n",
      "Batch number: 025, Training: Loss: 1.3502, Accuracy: 0.4375\n",
      "Batch number: 026, Training: Loss: 0.8550, Accuracy: 0.6875\n",
      "Batch number: 027, Training: Loss: 0.5780, Accuracy: 0.8750\n",
      "Batch number: 028, Training: Loss: 0.8546, Accuracy: 0.5625\n",
      "Batch number: 029, Training: Loss: 1.0350, Accuracy: 0.6250\n",
      "Batch number: 030, Training: Loss: 0.9748, Accuracy: 0.6250\n",
      "Batch number: 031, Training: Loss: 1.2972, Accuracy: 0.5000\n",
      "Batch number: 032, Training: Loss: 0.8999, Accuracy: 0.6250\n",
      "Batch number: 033, Training: Loss: 1.2549, Accuracy: 0.3750\n",
      "Batch number: 034, Training: Loss: 0.8094, Accuracy: 0.7500\n",
      "Batch number: 035, Training: Loss: 1.2949, Accuracy: 0.5000\n",
      "Batch number: 036, Training: Loss: 0.7708, Accuracy: 0.6875\n",
      "Batch number: 037, Training: Loss: 1.0571, Accuracy: 0.5625\n",
      "Batch number: 038, Training: Loss: 1.1654, Accuracy: 0.5625\n",
      "Batch number: 039, Training: Loss: 0.7687, Accuracy: 0.7500\n",
      "Batch number: 040, Training: Loss: 1.2579, Accuracy: 0.3125\n",
      "Batch number: 041, Training: Loss: 1.0422, Accuracy: 0.5000\n",
      "Batch number: 042, Training: Loss: 0.6572, Accuracy: 0.9375\n",
      "Batch number: 043, Training: Loss: 0.7880, Accuracy: 0.6875\n",
      "Batch number: 044, Training: Loss: 1.0321, Accuracy: 0.5625\n",
      "Batch number: 045, Training: Loss: 1.1506, Accuracy: 0.3750\n",
      "Batch number: 046, Training: Loss: 1.4231, Accuracy: 0.5000\n",
      "Batch number: 047, Training: Loss: 0.9880, Accuracy: 0.5625\n",
      "Batch number: 048, Training: Loss: 0.7096, Accuracy: 0.8125\n",
      "Batch number: 049, Training: Loss: 0.7624, Accuracy: 0.8750\n",
      "Batch number: 050, Training: Loss: 0.8628, Accuracy: 0.6250\n",
      "Batch number: 051, Training: Loss: 0.9522, Accuracy: 0.6875\n",
      "Batch number: 052, Training: Loss: 0.8626, Accuracy: 0.7500\n",
      "Batch number: 053, Training: Loss: 1.2036, Accuracy: 0.4375\n",
      "Batch number: 054, Training: Loss: 0.6229, Accuracy: 0.7500\n",
      "Batch number: 055, Training: Loss: 1.0764, Accuracy: 0.5625\n",
      "Batch number: 056, Training: Loss: 1.0712, Accuracy: 0.6250\n",
      "Batch number: 057, Training: Loss: 0.6930, Accuracy: 0.9375\n",
      "Batch number: 058, Training: Loss: 1.0494, Accuracy: 0.6250\n",
      "Batch number: 059, Training: Loss: 0.9510, Accuracy: 0.5625\n",
      "Batch number: 060, Training: Loss: 0.8008, Accuracy: 0.8750\n",
      "Batch number: 061, Training: Loss: 1.0465, Accuracy: 0.6250\n",
      "Batch number: 062, Training: Loss: 0.9878, Accuracy: 0.7500\n",
      "Batch number: 063, Training: Loss: 0.7595, Accuracy: 0.8125\n",
      "Batch number: 064, Training: Loss: 1.3268, Accuracy: 0.3750\n",
      "Batch number: 065, Training: Loss: 0.8634, Accuracy: 0.6875\n",
      "Batch number: 066, Training: Loss: 0.8897, Accuracy: 0.6875\n",
      "Batch number: 067, Training: Loss: 1.0236, Accuracy: 0.6875\n",
      "Batch number: 068, Training: Loss: 0.8437, Accuracy: 0.6875\n",
      "Batch number: 069, Training: Loss: 0.8012, Accuracy: 0.7500\n",
      "Batch number: 070, Training: Loss: 0.7353, Accuracy: 0.8125\n",
      "Batch number: 071, Training: Loss: 1.3642, Accuracy: 0.4375\n",
      "Batch number: 072, Training: Loss: 1.2112, Accuracy: 0.5000\n",
      "Batch number: 073, Training: Loss: 0.9006, Accuracy: 0.6250\n",
      "Batch number: 074, Training: Loss: 1.4823, Accuracy: 0.3750\n",
      "Batch number: 075, Training: Loss: 1.1078, Accuracy: 0.6250\n",
      "Batch number: 076, Training: Loss: 1.0688, Accuracy: 0.5625\n",
      "Batch number: 077, Training: Loss: 0.8090, Accuracy: 0.6250\n",
      "Batch number: 078, Training: Loss: 0.7603, Accuracy: 0.7500\n",
      "Batch number: 079, Training: Loss: 0.8672, Accuracy: 0.6250\n",
      "Batch number: 080, Training: Loss: 0.8199, Accuracy: 0.7500\n",
      "Batch number: 081, Training: Loss: 1.4744, Accuracy: 0.5000\n",
      "Batch number: 082, Training: Loss: 0.9213, Accuracy: 0.7500\n",
      "Batch number: 083, Training: Loss: 1.1427, Accuracy: 0.5625\n",
      "Batch number: 084, Training: Loss: 0.7806, Accuracy: 0.6250\n",
      "Batch number: 085, Training: Loss: 0.9402, Accuracy: 0.6250\n",
      "Batch number: 086, Training: Loss: 0.6069, Accuracy: 0.7500\n",
      "Batch number: 087, Training: Loss: 1.0808, Accuracy: 0.4375\n",
      "Batch number: 088, Training: Loss: 0.7117, Accuracy: 0.7500\n",
      "Batch number: 089, Training: Loss: 1.6067, Accuracy: 0.5000\n",
      "Batch number: 090, Training: Loss: 0.6328, Accuracy: 0.7500\n",
      "Batch number: 091, Training: Loss: 1.0622, Accuracy: 0.6875\n",
      "Batch number: 092, Training: Loss: 1.0475, Accuracy: 0.5625\n",
      "Batch number: 093, Training: Loss: 1.2238, Accuracy: 0.5625\n",
      "Batch number: 094, Training: Loss: 0.5498, Accuracy: 0.8750\n",
      "Batch number: 095, Training: Loss: 0.8173, Accuracy: 0.6250\n",
      "Batch number: 096, Training: Loss: 0.9563, Accuracy: 0.6250\n",
      "Batch number: 097, Training: Loss: 0.7647, Accuracy: 0.7500\n",
      "Batch number: 098, Training: Loss: 1.0034, Accuracy: 0.5000\n",
      "Batch number: 099, Training: Loss: 1.0652, Accuracy: 0.5000\n",
      "Batch number: 100, Training: Loss: 0.7641, Accuracy: 0.8750\n",
      "Batch number: 101, Training: Loss: 0.6152, Accuracy: 0.8125\n",
      "Batch number: 102, Training: Loss: 0.8207, Accuracy: 0.6875\n",
      "Batch number: 103, Training: Loss: 1.0662, Accuracy: 0.7500\n",
      "Batch number: 104, Training: Loss: 0.9515, Accuracy: 0.5000\n",
      "Batch number: 105, Training: Loss: 1.1952, Accuracy: 0.3750\n",
      "Batch number: 106, Training: Loss: 1.5609, Accuracy: 0.5000\n",
      "Batch number: 107, Training: Loss: 1.1221, Accuracy: 0.5625\n",
      "Batch number: 108, Training: Loss: 0.7260, Accuracy: 0.7500\n",
      "Batch number: 109, Training: Loss: 0.9550, Accuracy: 0.6250\n",
      "Batch number: 110, Training: Loss: 1.2362, Accuracy: 0.4375\n",
      "Batch number: 111, Training: Loss: 0.9099, Accuracy: 0.6250\n",
      "Batch number: 112, Training: Loss: 0.7050, Accuracy: 0.6250\n",
      "Batch number: 113, Training: Loss: 0.8984, Accuracy: 0.8125\n",
      "Batch number: 114, Training: Loss: 0.4848, Accuracy: 0.8750\n",
      "Batch number: 115, Training: Loss: 0.7038, Accuracy: 0.8125\n",
      "Batch number: 116, Training: Loss: 1.2623, Accuracy: 0.5000\n",
      "Batch number: 117, Training: Loss: 1.0965, Accuracy: 0.6250\n",
      "Batch number: 118, Training: Loss: 1.1538, Accuracy: 0.5625\n",
      "Batch number: 119, Training: Loss: 0.9966, Accuracy: 0.5625\n",
      "Batch number: 120, Training: Loss: 1.0339, Accuracy: 0.5000\n",
      "Batch number: 121, Training: Loss: 1.3781, Accuracy: 0.4375\n",
      "Batch number: 122, Training: Loss: 1.4178, Accuracy: 0.5000\n",
      "Batch number: 123, Training: Loss: 1.0854, Accuracy: 0.5000\n",
      "Batch number: 124, Training: Loss: 0.8390, Accuracy: 0.6250\n",
      "Batch number: 125, Training: Loss: 0.9179, Accuracy: 0.4375\n",
      "Batch number: 126, Training: Loss: 0.8422, Accuracy: 0.7500\n",
      "Batch number: 127, Training: Loss: 0.6257, Accuracy: 0.8125\n",
      "Batch number: 128, Training: Loss: 0.9106, Accuracy: 0.7500\n",
      "Batch number: 129, Training: Loss: 0.8416, Accuracy: 0.5625\n",
      "Batch number: 130, Training: Loss: 0.9408, Accuracy: 0.6875\n",
      "Batch number: 131, Training: Loss: 1.2486, Accuracy: 0.3750\n",
      "Batch number: 132, Training: Loss: 0.6254, Accuracy: 0.8125\n",
      "Batch number: 133, Training: Loss: 0.6727, Accuracy: 0.6875\n",
      "Batch number: 134, Training: Loss: 0.7629, Accuracy: 0.8125\n",
      "Batch number: 135, Training: Loss: 0.7405, Accuracy: 0.6875\n",
      "Batch number: 136, Training: Loss: 1.3815, Accuracy: 0.5000\n",
      "Batch number: 137, Training: Loss: 0.9474, Accuracy: 0.6875\n",
      "Batch number: 138, Training: Loss: 0.8564, Accuracy: 0.6875\n",
      "Batch number: 139, Training: Loss: 0.8108, Accuracy: 0.6875\n",
      "Batch number: 140, Training: Loss: 0.7443, Accuracy: 0.7500\n",
      "Batch number: 141, Training: Loss: 0.8781, Accuracy: 0.6250\n",
      "Batch number: 142, Training: Loss: 0.8693, Accuracy: 0.6250\n",
      "Batch number: 143, Training: Loss: 1.1072, Accuracy: 0.5000\n",
      "Batch number: 144, Training: Loss: 1.0740, Accuracy: 0.7500\n",
      "Batch number: 145, Training: Loss: 0.9077, Accuracy: 0.6875\n",
      "Batch number: 146, Training: Loss: 1.0125, Accuracy: 0.6875\n",
      "Batch number: 147, Training: Loss: 0.8208, Accuracy: 0.7500\n",
      "Batch number: 148, Training: Loss: 1.1287, Accuracy: 0.6250\n",
      "Batch number: 149, Training: Loss: 1.2184, Accuracy: 0.5625\n",
      "Batch number: 150, Training: Loss: 0.8828, Accuracy: 0.5625\n",
      "Batch number: 151, Training: Loss: 1.1923, Accuracy: 0.6250\n",
      "Batch number: 152, Training: Loss: 0.7511, Accuracy: 0.5625\n",
      "Batch number: 153, Training: Loss: 0.6087, Accuracy: 0.8750\n",
      "Batch number: 154, Training: Loss: 1.3007, Accuracy: 0.0000\n",
      "Epoch: 8/10\n",
      "Batch number: 000, Training: Loss: 0.8868, Accuracy: 0.5625\n",
      "Batch number: 001, Training: Loss: 0.8005, Accuracy: 0.6250\n",
      "Batch number: 002, Training: Loss: 1.0007, Accuracy: 0.6875\n",
      "Batch number: 003, Training: Loss: 0.9524, Accuracy: 0.7500\n",
      "Batch number: 004, Training: Loss: 0.8041, Accuracy: 0.6875\n",
      "Batch number: 005, Training: Loss: 1.2524, Accuracy: 0.5625\n",
      "Batch number: 006, Training: Loss: 1.0897, Accuracy: 0.6875\n",
      "Batch number: 007, Training: Loss: 0.8699, Accuracy: 0.6875\n",
      "Batch number: 008, Training: Loss: 1.0085, Accuracy: 0.5625\n",
      "Batch number: 009, Training: Loss: 0.8981, Accuracy: 0.7500\n",
      "Batch number: 010, Training: Loss: 1.0139, Accuracy: 0.5625\n",
      "Batch number: 011, Training: Loss: 1.0756, Accuracy: 0.6250\n",
      "Batch number: 012, Training: Loss: 0.7665, Accuracy: 0.7500\n",
      "Batch number: 013, Training: Loss: 1.0595, Accuracy: 0.5000\n",
      "Batch number: 014, Training: Loss: 0.9277, Accuracy: 0.6250\n",
      "Batch number: 015, Training: Loss: 0.8657, Accuracy: 0.6250\n",
      "Batch number: 016, Training: Loss: 0.9381, Accuracy: 0.7500\n",
      "Batch number: 017, Training: Loss: 1.1413, Accuracy: 0.5625\n",
      "Batch number: 018, Training: Loss: 0.9059, Accuracy: 0.6250\n",
      "Batch number: 019, Training: Loss: 0.6078, Accuracy: 0.8125\n",
      "Batch number: 020, Training: Loss: 1.3490, Accuracy: 0.5000\n",
      "Batch number: 021, Training: Loss: 1.0111, Accuracy: 0.6250\n",
      "Batch number: 022, Training: Loss: 1.3003, Accuracy: 0.6875\n",
      "Batch number: 023, Training: Loss: 0.8554, Accuracy: 0.6875\n",
      "Batch number: 024, Training: Loss: 0.7522, Accuracy: 0.6875\n",
      "Batch number: 025, Training: Loss: 1.1740, Accuracy: 0.5625\n",
      "Batch number: 026, Training: Loss: 0.8927, Accuracy: 0.6250\n",
      "Batch number: 027, Training: Loss: 1.0017, Accuracy: 0.5625\n",
      "Batch number: 028, Training: Loss: 0.8143, Accuracy: 0.6875\n",
      "Batch number: 029, Training: Loss: 0.6881, Accuracy: 0.8125\n",
      "Batch number: 030, Training: Loss: 1.0766, Accuracy: 0.4375\n",
      "Batch number: 031, Training: Loss: 0.9577, Accuracy: 0.6875\n",
      "Batch number: 032, Training: Loss: 1.0271, Accuracy: 0.6875\n",
      "Batch number: 033, Training: Loss: 1.4880, Accuracy: 0.3750\n",
      "Batch number: 034, Training: Loss: 0.8590, Accuracy: 0.7500\n",
      "Batch number: 035, Training: Loss: 1.2334, Accuracy: 0.6250\n",
      "Batch number: 036, Training: Loss: 0.9082, Accuracy: 0.6875\n",
      "Batch number: 037, Training: Loss: 1.1357, Accuracy: 0.5625\n",
      "Batch number: 038, Training: Loss: 1.3082, Accuracy: 0.4375\n",
      "Batch number: 039, Training: Loss: 0.5839, Accuracy: 0.8125\n",
      "Batch number: 040, Training: Loss: 1.1049, Accuracy: 0.4375\n",
      "Batch number: 041, Training: Loss: 0.8053, Accuracy: 0.6875\n",
      "Batch number: 042, Training: Loss: 0.7498, Accuracy: 0.7500\n",
      "Batch number: 043, Training: Loss: 0.7298, Accuracy: 0.8750\n",
      "Batch number: 044, Training: Loss: 0.9991, Accuracy: 0.6875\n",
      "Batch number: 045, Training: Loss: 1.2561, Accuracy: 0.3750\n",
      "Batch number: 046, Training: Loss: 0.8948, Accuracy: 0.6250\n",
      "Batch number: 047, Training: Loss: 1.0609, Accuracy: 0.6250\n",
      "Batch number: 048, Training: Loss: 1.3179, Accuracy: 0.4375\n",
      "Batch number: 049, Training: Loss: 1.2462, Accuracy: 0.5625\n",
      "Batch number: 050, Training: Loss: 0.8065, Accuracy: 0.8125\n",
      "Batch number: 051, Training: Loss: 1.1228, Accuracy: 0.5000\n",
      "Batch number: 052, Training: Loss: 0.7747, Accuracy: 0.8750\n",
      "Batch number: 053, Training: Loss: 0.7257, Accuracy: 0.6875\n",
      "Batch number: 054, Training: Loss: 0.7559, Accuracy: 0.7500\n",
      "Batch number: 055, Training: Loss: 0.8205, Accuracy: 0.7500\n",
      "Batch number: 056, Training: Loss: 0.6438, Accuracy: 0.7500\n",
      "Batch number: 057, Training: Loss: 1.3155, Accuracy: 0.6250\n",
      "Batch number: 058, Training: Loss: 0.8403, Accuracy: 0.7500\n",
      "Batch number: 059, Training: Loss: 0.7483, Accuracy: 0.6250\n",
      "Batch number: 060, Training: Loss: 0.9979, Accuracy: 0.5625\n",
      "Batch number: 061, Training: Loss: 0.9732, Accuracy: 0.5000\n",
      "Batch number: 062, Training: Loss: 0.8369, Accuracy: 0.7500\n",
      "Batch number: 063, Training: Loss: 0.9093, Accuracy: 0.6875\n",
      "Batch number: 064, Training: Loss: 1.1850, Accuracy: 0.5625\n",
      "Batch number: 065, Training: Loss: 0.6424, Accuracy: 0.6875\n",
      "Batch number: 066, Training: Loss: 0.8486, Accuracy: 0.5625\n",
      "Batch number: 067, Training: Loss: 0.7458, Accuracy: 0.8125\n",
      "Batch number: 068, Training: Loss: 1.1496, Accuracy: 0.6875\n",
      "Batch number: 069, Training: Loss: 1.0512, Accuracy: 0.6250\n",
      "Batch number: 070, Training: Loss: 1.1278, Accuracy: 0.6875\n",
      "Batch number: 071, Training: Loss: 1.2432, Accuracy: 0.5625\n",
      "Batch number: 072, Training: Loss: 0.9895, Accuracy: 0.6250\n",
      "Batch number: 073, Training: Loss: 0.5136, Accuracy: 0.9375\n",
      "Batch number: 074, Training: Loss: 1.1585, Accuracy: 0.5000\n",
      "Batch number: 075, Training: Loss: 0.8584, Accuracy: 0.6250\n",
      "Batch number: 076, Training: Loss: 1.0675, Accuracy: 0.5000\n",
      "Batch number: 077, Training: Loss: 0.7869, Accuracy: 0.8125\n",
      "Batch number: 078, Training: Loss: 0.8938, Accuracy: 0.6250\n",
      "Batch number: 079, Training: Loss: 1.0983, Accuracy: 0.5000\n",
      "Batch number: 080, Training: Loss: 0.6784, Accuracy: 0.8125\n",
      "Batch number: 081, Training: Loss: 1.0263, Accuracy: 0.5625\n",
      "Batch number: 082, Training: Loss: 0.7836, Accuracy: 0.8125\n",
      "Batch number: 083, Training: Loss: 0.8560, Accuracy: 0.7500\n",
      "Batch number: 084, Training: Loss: 0.9255, Accuracy: 0.6250\n",
      "Batch number: 085, Training: Loss: 0.9419, Accuracy: 0.5625\n",
      "Batch number: 086, Training: Loss: 0.9491, Accuracy: 0.6250\n",
      "Batch number: 087, Training: Loss: 1.1037, Accuracy: 0.5000\n",
      "Batch number: 088, Training: Loss: 0.6188, Accuracy: 0.8750\n",
      "Batch number: 089, Training: Loss: 0.9164, Accuracy: 0.5625\n",
      "Batch number: 090, Training: Loss: 0.8064, Accuracy: 0.5625\n",
      "Batch number: 091, Training: Loss: 0.8855, Accuracy: 0.5625\n",
      "Batch number: 092, Training: Loss: 0.6033, Accuracy: 0.8125\n",
      "Batch number: 093, Training: Loss: 0.7418, Accuracy: 0.7500\n",
      "Batch number: 094, Training: Loss: 1.0812, Accuracy: 0.5625\n",
      "Batch number: 095, Training: Loss: 0.6768, Accuracy: 0.8125\n",
      "Batch number: 096, Training: Loss: 1.3434, Accuracy: 0.5625\n",
      "Batch number: 097, Training: Loss: 0.6813, Accuracy: 0.8125\n",
      "Batch number: 098, Training: Loss: 0.7170, Accuracy: 0.7500\n",
      "Batch number: 099, Training: Loss: 0.4316, Accuracy: 0.8125\n",
      "Batch number: 100, Training: Loss: 0.8773, Accuracy: 0.7500\n",
      "Batch number: 101, Training: Loss: 1.3973, Accuracy: 0.4375\n",
      "Batch number: 102, Training: Loss: 0.7111, Accuracy: 0.6875\n",
      "Batch number: 103, Training: Loss: 0.8533, Accuracy: 0.6250\n",
      "Batch number: 104, Training: Loss: 0.9454, Accuracy: 0.5625\n",
      "Batch number: 105, Training: Loss: 0.9758, Accuracy: 0.6250\n",
      "Batch number: 106, Training: Loss: 0.9684, Accuracy: 0.6250\n",
      "Batch number: 107, Training: Loss: 0.8036, Accuracy: 0.6250\n",
      "Batch number: 108, Training: Loss: 1.0727, Accuracy: 0.6250\n",
      "Batch number: 109, Training: Loss: 1.0734, Accuracy: 0.6250\n",
      "Batch number: 110, Training: Loss: 1.3806, Accuracy: 0.5625\n",
      "Batch number: 111, Training: Loss: 1.3227, Accuracy: 0.5000\n",
      "Batch number: 112, Training: Loss: 0.8823, Accuracy: 0.7500\n",
      "Batch number: 113, Training: Loss: 0.5608, Accuracy: 0.8125\n",
      "Batch number: 114, Training: Loss: 0.6125, Accuracy: 0.7500\n",
      "Batch number: 115, Training: Loss: 0.7678, Accuracy: 0.6875\n",
      "Batch number: 116, Training: Loss: 0.7074, Accuracy: 0.6875\n",
      "Batch number: 117, Training: Loss: 0.9112, Accuracy: 0.8125\n",
      "Batch number: 118, Training: Loss: 0.6721, Accuracy: 0.8750\n",
      "Batch number: 119, Training: Loss: 0.9334, Accuracy: 0.7500\n",
      "Batch number: 120, Training: Loss: 0.8652, Accuracy: 0.6875\n",
      "Batch number: 121, Training: Loss: 0.6308, Accuracy: 0.8750\n",
      "Batch number: 122, Training: Loss: 1.1664, Accuracy: 0.5000\n",
      "Batch number: 123, Training: Loss: 0.9197, Accuracy: 0.6250\n",
      "Batch number: 124, Training: Loss: 0.9221, Accuracy: 0.5625\n",
      "Batch number: 125, Training: Loss: 0.9880, Accuracy: 0.5625\n",
      "Batch number: 126, Training: Loss: 0.6668, Accuracy: 0.8125\n",
      "Batch number: 127, Training: Loss: 0.9475, Accuracy: 0.6250\n",
      "Batch number: 128, Training: Loss: 0.3763, Accuracy: 0.8750\n",
      "Batch number: 129, Training: Loss: 1.0096, Accuracy: 0.5625\n",
      "Batch number: 130, Training: Loss: 0.8324, Accuracy: 0.7500\n",
      "Batch number: 131, Training: Loss: 1.0089, Accuracy: 0.6250\n",
      "Batch number: 132, Training: Loss: 1.6377, Accuracy: 0.4375\n",
      "Batch number: 133, Training: Loss: 0.7469, Accuracy: 0.7500\n",
      "Batch number: 134, Training: Loss: 1.2962, Accuracy: 0.5000\n",
      "Batch number: 135, Training: Loss: 0.9565, Accuracy: 0.6875\n",
      "Batch number: 136, Training: Loss: 0.8635, Accuracy: 0.6250\n",
      "Batch number: 137, Training: Loss: 1.1088, Accuracy: 0.5000\n",
      "Batch number: 138, Training: Loss: 0.7582, Accuracy: 0.6250\n",
      "Batch number: 139, Training: Loss: 0.9777, Accuracy: 0.5625\n",
      "Batch number: 140, Training: Loss: 1.0577, Accuracy: 0.6250\n",
      "Batch number: 141, Training: Loss: 1.1696, Accuracy: 0.5625\n",
      "Batch number: 142, Training: Loss: 1.3196, Accuracy: 0.5000\n",
      "Batch number: 143, Training: Loss: 1.1019, Accuracy: 0.5625\n",
      "Batch number: 144, Training: Loss: 1.4931, Accuracy: 0.5000\n",
      "Batch number: 145, Training: Loss: 1.0884, Accuracy: 0.5625\n",
      "Batch number: 146, Training: Loss: 1.2490, Accuracy: 0.5625\n",
      "Batch number: 147, Training: Loss: 1.2631, Accuracy: 0.5000\n",
      "Batch number: 148, Training: Loss: 1.1074, Accuracy: 0.6250\n",
      "Batch number: 149, Training: Loss: 0.4998, Accuracy: 0.8125\n",
      "Batch number: 150, Training: Loss: 0.9507, Accuracy: 0.6250\n",
      "Batch number: 151, Training: Loss: 0.8944, Accuracy: 0.6875\n",
      "Batch number: 152, Training: Loss: 1.4982, Accuracy: 0.4375\n",
      "Batch number: 153, Training: Loss: 1.0581, Accuracy: 0.6875\n",
      "Batch number: 154, Training: Loss: 1.0705, Accuracy: 0.6667\n",
      "Epoch: 9/10\n",
      "Batch number: 000, Training: Loss: 0.7058, Accuracy: 0.6875\n",
      "Batch number: 001, Training: Loss: 1.2078, Accuracy: 0.6250\n",
      "Batch number: 002, Training: Loss: 0.8448, Accuracy: 0.6250\n",
      "Batch number: 003, Training: Loss: 0.7270, Accuracy: 0.8125\n",
      "Batch number: 004, Training: Loss: 0.7860, Accuracy: 0.7500\n",
      "Batch number: 005, Training: Loss: 1.0943, Accuracy: 0.5625\n",
      "Batch number: 006, Training: Loss: 1.0110, Accuracy: 0.6875\n",
      "Batch number: 007, Training: Loss: 1.1521, Accuracy: 0.3750\n",
      "Batch number: 008, Training: Loss: 1.0757, Accuracy: 0.5625\n",
      "Batch number: 009, Training: Loss: 0.7790, Accuracy: 0.7500\n",
      "Batch number: 010, Training: Loss: 1.0800, Accuracy: 0.5000\n",
      "Batch number: 011, Training: Loss: 0.9806, Accuracy: 0.5000\n",
      "Batch number: 012, Training: Loss: 0.8492, Accuracy: 0.6875\n",
      "Batch number: 013, Training: Loss: 0.7725, Accuracy: 0.6875\n",
      "Batch number: 014, Training: Loss: 0.9764, Accuracy: 0.6875\n",
      "Batch number: 015, Training: Loss: 0.7015, Accuracy: 0.8750\n",
      "Batch number: 016, Training: Loss: 0.7166, Accuracy: 0.6875\n",
      "Batch number: 017, Training: Loss: 0.9100, Accuracy: 0.7500\n",
      "Batch number: 018, Training: Loss: 0.6369, Accuracy: 0.8125\n",
      "Batch number: 019, Training: Loss: 1.0902, Accuracy: 0.6250\n",
      "Batch number: 020, Training: Loss: 0.5458, Accuracy: 0.8750\n",
      "Batch number: 021, Training: Loss: 0.5895, Accuracy: 0.8125\n",
      "Batch number: 022, Training: Loss: 0.9102, Accuracy: 0.6875\n",
      "Batch number: 023, Training: Loss: 1.2203, Accuracy: 0.5625\n",
      "Batch number: 024, Training: Loss: 0.8862, Accuracy: 0.5000\n",
      "Batch number: 025, Training: Loss: 1.4623, Accuracy: 0.4375\n",
      "Batch number: 026, Training: Loss: 0.8920, Accuracy: 0.5625\n",
      "Batch number: 027, Training: Loss: 1.1852, Accuracy: 0.5000\n",
      "Batch number: 028, Training: Loss: 0.6711, Accuracy: 0.7500\n",
      "Batch number: 029, Training: Loss: 0.6968, Accuracy: 0.7500\n",
      "Batch number: 030, Training: Loss: 0.7933, Accuracy: 0.6875\n",
      "Batch number: 031, Training: Loss: 0.9015, Accuracy: 0.6875\n",
      "Batch number: 032, Training: Loss: 1.1593, Accuracy: 0.5625\n",
      "Batch number: 033, Training: Loss: 0.7459, Accuracy: 0.8125\n",
      "Batch number: 034, Training: Loss: 1.1052, Accuracy: 0.4375\n",
      "Batch number: 035, Training: Loss: 0.9062, Accuracy: 0.6875\n",
      "Batch number: 036, Training: Loss: 0.7993, Accuracy: 0.8125\n",
      "Batch number: 037, Training: Loss: 0.9937, Accuracy: 0.5625\n",
      "Batch number: 038, Training: Loss: 0.8175, Accuracy: 0.7500\n",
      "Batch number: 039, Training: Loss: 1.5959, Accuracy: 0.4375\n",
      "Batch number: 040, Training: Loss: 0.9003, Accuracy: 0.5625\n",
      "Batch number: 041, Training: Loss: 0.3947, Accuracy: 0.9375\n",
      "Batch number: 042, Training: Loss: 0.9006, Accuracy: 0.7500\n",
      "Batch number: 043, Training: Loss: 1.0631, Accuracy: 0.6250\n",
      "Batch number: 044, Training: Loss: 1.4261, Accuracy: 0.5000\n",
      "Batch number: 045, Training: Loss: 0.7784, Accuracy: 0.8125\n",
      "Batch number: 046, Training: Loss: 1.0570, Accuracy: 0.6250\n",
      "Batch number: 047, Training: Loss: 0.9677, Accuracy: 0.6250\n",
      "Batch number: 048, Training: Loss: 1.0076, Accuracy: 0.5625\n",
      "Batch number: 049, Training: Loss: 0.9912, Accuracy: 0.6875\n",
      "Batch number: 050, Training: Loss: 0.5529, Accuracy: 0.8750\n",
      "Batch number: 051, Training: Loss: 0.9909, Accuracy: 0.5625\n",
      "Batch number: 052, Training: Loss: 1.1303, Accuracy: 0.5625\n",
      "Batch number: 053, Training: Loss: 0.5875, Accuracy: 0.7500\n",
      "Batch number: 054, Training: Loss: 0.9763, Accuracy: 0.6250\n",
      "Batch number: 055, Training: Loss: 1.0796, Accuracy: 0.5000\n",
      "Batch number: 056, Training: Loss: 0.9764, Accuracy: 0.6250\n",
      "Batch number: 057, Training: Loss: 1.2032, Accuracy: 0.6250\n",
      "Batch number: 058, Training: Loss: 0.7646, Accuracy: 0.6875\n",
      "Batch number: 059, Training: Loss: 0.9155, Accuracy: 0.6250\n",
      "Batch number: 060, Training: Loss: 1.4194, Accuracy: 0.5000\n",
      "Batch number: 061, Training: Loss: 0.5766, Accuracy: 0.7500\n",
      "Batch number: 062, Training: Loss: 0.7156, Accuracy: 0.8125\n",
      "Batch number: 063, Training: Loss: 1.0166, Accuracy: 0.6250\n",
      "Batch number: 064, Training: Loss: 1.0063, Accuracy: 0.6250\n",
      "Batch number: 065, Training: Loss: 1.0880, Accuracy: 0.6250\n",
      "Batch number: 066, Training: Loss: 1.1383, Accuracy: 0.5625\n",
      "Batch number: 067, Training: Loss: 0.9820, Accuracy: 0.6250\n",
      "Batch number: 068, Training: Loss: 0.9874, Accuracy: 0.6875\n",
      "Batch number: 069, Training: Loss: 0.8632, Accuracy: 0.6250\n",
      "Batch number: 070, Training: Loss: 0.8729, Accuracy: 0.6250\n",
      "Batch number: 071, Training: Loss: 1.2157, Accuracy: 0.5625\n",
      "Batch number: 072, Training: Loss: 1.2708, Accuracy: 0.5625\n",
      "Batch number: 073, Training: Loss: 1.0061, Accuracy: 0.6250\n",
      "Batch number: 074, Training: Loss: 1.0147, Accuracy: 0.5625\n",
      "Batch number: 075, Training: Loss: 0.6531, Accuracy: 0.8750\n",
      "Batch number: 076, Training: Loss: 1.3007, Accuracy: 0.5625\n",
      "Batch number: 077, Training: Loss: 0.9622, Accuracy: 0.6250\n",
      "Batch number: 078, Training: Loss: 0.7685, Accuracy: 0.6875\n",
      "Batch number: 079, Training: Loss: 0.9197, Accuracy: 0.7500\n",
      "Batch number: 080, Training: Loss: 0.7280, Accuracy: 0.6875\n",
      "Batch number: 081, Training: Loss: 0.6801, Accuracy: 0.7500\n",
      "Batch number: 082, Training: Loss: 1.0564, Accuracy: 0.5000\n",
      "Batch number: 083, Training: Loss: 1.2233, Accuracy: 0.5000\n",
      "Batch number: 084, Training: Loss: 0.8661, Accuracy: 0.6250\n",
      "Batch number: 085, Training: Loss: 0.9026, Accuracy: 0.6875\n",
      "Batch number: 086, Training: Loss: 1.1450, Accuracy: 0.6875\n",
      "Batch number: 087, Training: Loss: 0.9064, Accuracy: 0.7500\n",
      "Batch number: 088, Training: Loss: 1.5680, Accuracy: 0.3125\n",
      "Batch number: 089, Training: Loss: 1.0233, Accuracy: 0.4375\n",
      "Batch number: 090, Training: Loss: 0.8642, Accuracy: 0.5625\n",
      "Batch number: 091, Training: Loss: 0.9288, Accuracy: 0.6250\n",
      "Batch number: 092, Training: Loss: 0.7947, Accuracy: 0.8125\n",
      "Batch number: 093, Training: Loss: 0.5889, Accuracy: 0.8125\n",
      "Batch number: 094, Training: Loss: 1.0071, Accuracy: 0.5625\n",
      "Batch number: 095, Training: Loss: 0.8412, Accuracy: 0.6875\n",
      "Batch number: 096, Training: Loss: 1.3968, Accuracy: 0.4375\n",
      "Batch number: 097, Training: Loss: 0.9514, Accuracy: 0.6250\n",
      "Batch number: 098, Training: Loss: 0.9088, Accuracy: 0.6875\n",
      "Batch number: 099, Training: Loss: 0.9565, Accuracy: 0.7500\n",
      "Batch number: 100, Training: Loss: 1.1962, Accuracy: 0.5625\n",
      "Batch number: 101, Training: Loss: 0.8623, Accuracy: 0.6875\n",
      "Batch number: 102, Training: Loss: 0.9062, Accuracy: 0.6875\n",
      "Batch number: 103, Training: Loss: 0.7007, Accuracy: 0.6875\n",
      "Batch number: 104, Training: Loss: 0.8399, Accuracy: 0.6875\n",
      "Batch number: 105, Training: Loss: 0.8983, Accuracy: 0.6250\n",
      "Batch number: 106, Training: Loss: 1.1816, Accuracy: 0.4375\n",
      "Batch number: 107, Training: Loss: 1.0109, Accuracy: 0.6250\n",
      "Batch number: 108, Training: Loss: 1.4903, Accuracy: 0.5000\n",
      "Batch number: 109, Training: Loss: 0.7231, Accuracy: 0.8750\n",
      "Batch number: 110, Training: Loss: 0.8132, Accuracy: 0.8125\n",
      "Batch number: 111, Training: Loss: 0.9539, Accuracy: 0.6875\n",
      "Batch number: 112, Training: Loss: 0.9521, Accuracy: 0.5625\n",
      "Batch number: 113, Training: Loss: 0.6911, Accuracy: 0.8125\n",
      "Batch number: 114, Training: Loss: 0.9050, Accuracy: 0.6250\n",
      "Batch number: 115, Training: Loss: 0.9068, Accuracy: 0.6875\n",
      "Batch number: 116, Training: Loss: 1.0932, Accuracy: 0.6250\n",
      "Batch number: 117, Training: Loss: 0.7934, Accuracy: 0.7500\n",
      "Batch number: 118, Training: Loss: 1.1768, Accuracy: 0.6875\n",
      "Batch number: 119, Training: Loss: 0.8555, Accuracy: 0.6875\n",
      "Batch number: 120, Training: Loss: 0.9601, Accuracy: 0.6875\n",
      "Batch number: 121, Training: Loss: 0.7782, Accuracy: 0.6875\n",
      "Batch number: 122, Training: Loss: 1.1602, Accuracy: 0.5625\n",
      "Batch number: 123, Training: Loss: 0.7443, Accuracy: 0.8125\n",
      "Batch number: 124, Training: Loss: 0.6015, Accuracy: 0.8125\n",
      "Batch number: 125, Training: Loss: 1.5625, Accuracy: 0.5000\n",
      "Batch number: 126, Training: Loss: 0.4989, Accuracy: 0.7500\n",
      "Batch number: 127, Training: Loss: 0.8372, Accuracy: 0.6250\n",
      "Batch number: 128, Training: Loss: 1.0232, Accuracy: 0.5625\n",
      "Batch number: 129, Training: Loss: 0.7179, Accuracy: 0.7500\n",
      "Batch number: 130, Training: Loss: 0.9384, Accuracy: 0.5625\n",
      "Batch number: 131, Training: Loss: 0.9188, Accuracy: 0.6250\n",
      "Batch number: 132, Training: Loss: 1.0143, Accuracy: 0.7500\n",
      "Batch number: 133, Training: Loss: 0.6076, Accuracy: 0.8125\n",
      "Batch number: 134, Training: Loss: 0.7970, Accuracy: 0.8125\n",
      "Batch number: 135, Training: Loss: 1.3747, Accuracy: 0.3125\n",
      "Batch number: 136, Training: Loss: 0.8628, Accuracy: 0.6875\n",
      "Batch number: 137, Training: Loss: 1.0397, Accuracy: 0.5625\n",
      "Batch number: 138, Training: Loss: 0.5853, Accuracy: 0.8125\n",
      "Batch number: 139, Training: Loss: 0.6877, Accuracy: 0.6875\n",
      "Batch number: 140, Training: Loss: 1.2142, Accuracy: 0.5000\n",
      "Batch number: 141, Training: Loss: 0.8940, Accuracy: 0.7500\n",
      "Batch number: 142, Training: Loss: 1.0655, Accuracy: 0.6250\n",
      "Batch number: 143, Training: Loss: 1.1510, Accuracy: 0.5625\n",
      "Batch number: 144, Training: Loss: 0.9255, Accuracy: 0.5625\n",
      "Batch number: 145, Training: Loss: 1.0743, Accuracy: 0.5000\n",
      "Batch number: 146, Training: Loss: 0.7026, Accuracy: 0.8125\n",
      "Batch number: 147, Training: Loss: 1.0524, Accuracy: 0.6250\n",
      "Batch number: 148, Training: Loss: 1.0295, Accuracy: 0.5625\n",
      "Batch number: 149, Training: Loss: 0.7910, Accuracy: 0.7500\n",
      "Batch number: 150, Training: Loss: 0.7484, Accuracy: 0.6875\n",
      "Batch number: 151, Training: Loss: 0.7975, Accuracy: 0.7500\n",
      "Batch number: 152, Training: Loss: 0.7569, Accuracy: 0.7500\n",
      "Batch number: 153, Training: Loss: 0.9396, Accuracy: 0.6250\n",
      "Batch number: 154, Training: Loss: 0.8652, Accuracy: 1.0000\n",
      "Epoch: 10/10\n",
      "Batch number: 000, Training: Loss: 1.1080, Accuracy: 0.5625\n",
      "Batch number: 001, Training: Loss: 1.1280, Accuracy: 0.6875\n",
      "Batch number: 002, Training: Loss: 0.9766, Accuracy: 0.5625\n",
      "Batch number: 003, Training: Loss: 0.6653, Accuracy: 0.6250\n",
      "Batch number: 004, Training: Loss: 1.0074, Accuracy: 0.5000\n",
      "Batch number: 005, Training: Loss: 1.1829, Accuracy: 0.5000\n",
      "Batch number: 006, Training: Loss: 0.8257, Accuracy: 0.6250\n",
      "Batch number: 007, Training: Loss: 0.6954, Accuracy: 0.6875\n",
      "Batch number: 008, Training: Loss: 1.0981, Accuracy: 0.5000\n",
      "Batch number: 009, Training: Loss: 0.9497, Accuracy: 0.6250\n",
      "Batch number: 010, Training: Loss: 1.0324, Accuracy: 0.6250\n",
      "Batch number: 011, Training: Loss: 0.8140, Accuracy: 0.6250\n",
      "Batch number: 012, Training: Loss: 0.7218, Accuracy: 0.8125\n",
      "Batch number: 013, Training: Loss: 0.8118, Accuracy: 0.6250\n",
      "Batch number: 014, Training: Loss: 0.9843, Accuracy: 0.5625\n",
      "Batch number: 015, Training: Loss: 0.9956, Accuracy: 0.8125\n",
      "Batch number: 016, Training: Loss: 0.9993, Accuracy: 0.6250\n",
      "Batch number: 017, Training: Loss: 1.3986, Accuracy: 0.5000\n",
      "Batch number: 018, Training: Loss: 0.8040, Accuracy: 0.8125\n",
      "Batch number: 019, Training: Loss: 0.7869, Accuracy: 0.8125\n",
      "Batch number: 020, Training: Loss: 0.8348, Accuracy: 0.6250\n",
      "Batch number: 021, Training: Loss: 1.6425, Accuracy: 0.3750\n",
      "Batch number: 022, Training: Loss: 0.7458, Accuracy: 0.7500\n",
      "Batch number: 023, Training: Loss: 0.9191, Accuracy: 0.6250\n",
      "Batch number: 024, Training: Loss: 0.4828, Accuracy: 0.9375\n",
      "Batch number: 025, Training: Loss: 1.0359, Accuracy: 0.5625\n",
      "Batch number: 026, Training: Loss: 1.0571, Accuracy: 0.5000\n",
      "Batch number: 027, Training: Loss: 0.5963, Accuracy: 0.8125\n",
      "Batch number: 028, Training: Loss: 1.0325, Accuracy: 0.6250\n",
      "Batch number: 029, Training: Loss: 1.1353, Accuracy: 0.5625\n",
      "Batch number: 030, Training: Loss: 0.8019, Accuracy: 0.6875\n",
      "Batch number: 031, Training: Loss: 0.7296, Accuracy: 0.8125\n",
      "Batch number: 032, Training: Loss: 0.7575, Accuracy: 0.7500\n",
      "Batch number: 033, Training: Loss: 0.8596, Accuracy: 0.6875\n",
      "Batch number: 034, Training: Loss: 1.1851, Accuracy: 0.5625\n",
      "Batch number: 035, Training: Loss: 1.2078, Accuracy: 0.5625\n",
      "Batch number: 036, Training: Loss: 0.8334, Accuracy: 0.7500\n",
      "Batch number: 037, Training: Loss: 1.1708, Accuracy: 0.5000\n",
      "Batch number: 038, Training: Loss: 0.9617, Accuracy: 0.6250\n",
      "Batch number: 039, Training: Loss: 0.5209, Accuracy: 0.8125\n",
      "Batch number: 040, Training: Loss: 1.1235, Accuracy: 0.6250\n",
      "Batch number: 041, Training: Loss: 1.1192, Accuracy: 0.6250\n",
      "Batch number: 042, Training: Loss: 0.9797, Accuracy: 0.4375\n",
      "Batch number: 043, Training: Loss: 0.7524, Accuracy: 0.8750\n",
      "Batch number: 044, Training: Loss: 0.7314, Accuracy: 0.7500\n",
      "Batch number: 045, Training: Loss: 0.9338, Accuracy: 0.6875\n",
      "Batch number: 046, Training: Loss: 0.9053, Accuracy: 0.6250\n",
      "Batch number: 047, Training: Loss: 0.9203, Accuracy: 0.7500\n",
      "Batch number: 048, Training: Loss: 1.2173, Accuracy: 0.6875\n",
      "Batch number: 049, Training: Loss: 1.2713, Accuracy: 0.5000\n",
      "Batch number: 050, Training: Loss: 0.9352, Accuracy: 0.7500\n",
      "Batch number: 051, Training: Loss: 1.4365, Accuracy: 0.4375\n",
      "Batch number: 052, Training: Loss: 0.7583, Accuracy: 0.6875\n",
      "Batch number: 053, Training: Loss: 1.0203, Accuracy: 0.5625\n",
      "Batch number: 054, Training: Loss: 0.9101, Accuracy: 0.5625\n",
      "Batch number: 055, Training: Loss: 0.5875, Accuracy: 0.8750\n",
      "Batch number: 056, Training: Loss: 0.9599, Accuracy: 0.6250\n",
      "Batch number: 057, Training: Loss: 0.8583, Accuracy: 0.6875\n",
      "Batch number: 058, Training: Loss: 1.0608, Accuracy: 0.5000\n",
      "Batch number: 059, Training: Loss: 0.7815, Accuracy: 0.6250\n",
      "Batch number: 060, Training: Loss: 0.6293, Accuracy: 0.7500\n",
      "Batch number: 061, Training: Loss: 0.8411, Accuracy: 0.5625\n",
      "Batch number: 062, Training: Loss: 0.7400, Accuracy: 0.6875\n",
      "Batch number: 063, Training: Loss: 0.5328, Accuracy: 0.8125\n",
      "Batch number: 064, Training: Loss: 1.0791, Accuracy: 0.6250\n",
      "Batch number: 065, Training: Loss: 1.3384, Accuracy: 0.4375\n",
      "Batch number: 066, Training: Loss: 0.6319, Accuracy: 0.7500\n",
      "Batch number: 067, Training: Loss: 0.5750, Accuracy: 0.7500\n",
      "Batch number: 068, Training: Loss: 1.0255, Accuracy: 0.5000\n",
      "Batch number: 069, Training: Loss: 1.1560, Accuracy: 0.5625\n",
      "Batch number: 070, Training: Loss: 1.5902, Accuracy: 0.5000\n",
      "Batch number: 071, Training: Loss: 1.2431, Accuracy: 0.4375\n",
      "Batch number: 072, Training: Loss: 0.8144, Accuracy: 0.7500\n",
      "Batch number: 073, Training: Loss: 1.3945, Accuracy: 0.5000\n",
      "Batch number: 074, Training: Loss: 0.8553, Accuracy: 0.7500\n",
      "Batch number: 075, Training: Loss: 0.7016, Accuracy: 0.7500\n",
      "Batch number: 076, Training: Loss: 0.8080, Accuracy: 0.6250\n",
      "Batch number: 077, Training: Loss: 0.7589, Accuracy: 0.8125\n",
      "Batch number: 078, Training: Loss: 1.2453, Accuracy: 0.5000\n",
      "Batch number: 079, Training: Loss: 0.8304, Accuracy: 0.7500\n",
      "Batch number: 080, Training: Loss: 1.2073, Accuracy: 0.5625\n",
      "Batch number: 081, Training: Loss: 0.8167, Accuracy: 0.6875\n",
      "Batch number: 082, Training: Loss: 1.0576, Accuracy: 0.6250\n",
      "Batch number: 083, Training: Loss: 0.8884, Accuracy: 0.7500\n",
      "Batch number: 084, Training: Loss: 1.0938, Accuracy: 0.6250\n",
      "Batch number: 085, Training: Loss: 1.0144, Accuracy: 0.6250\n",
      "Batch number: 086, Training: Loss: 1.0028, Accuracy: 0.5000\n",
      "Batch number: 087, Training: Loss: 0.9879, Accuracy: 0.6875\n",
      "Batch number: 088, Training: Loss: 1.1636, Accuracy: 0.6250\n",
      "Batch number: 089, Training: Loss: 0.9071, Accuracy: 0.7500\n",
      "Batch number: 090, Training: Loss: 0.9717, Accuracy: 0.6250\n",
      "Batch number: 091, Training: Loss: 0.5997, Accuracy: 0.8125\n",
      "Batch number: 092, Training: Loss: 1.0083, Accuracy: 0.6250\n",
      "Batch number: 093, Training: Loss: 0.9125, Accuracy: 0.6250\n",
      "Batch number: 094, Training: Loss: 0.4484, Accuracy: 0.9375\n",
      "Batch number: 095, Training: Loss: 1.1156, Accuracy: 0.6875\n",
      "Batch number: 096, Training: Loss: 1.0110, Accuracy: 0.6875\n",
      "Batch number: 097, Training: Loss: 2.0084, Accuracy: 0.1875\n",
      "Batch number: 098, Training: Loss: 0.7916, Accuracy: 0.7500\n",
      "Batch number: 099, Training: Loss: 1.0006, Accuracy: 0.6250\n",
      "Batch number: 100, Training: Loss: 1.0757, Accuracy: 0.4375\n",
      "Batch number: 101, Training: Loss: 0.7086, Accuracy: 0.8125\n",
      "Batch number: 102, Training: Loss: 1.0445, Accuracy: 0.5000\n",
      "Batch number: 103, Training: Loss: 1.2479, Accuracy: 0.5000\n",
      "Batch number: 104, Training: Loss: 0.7956, Accuracy: 0.7500\n",
      "Batch number: 105, Training: Loss: 0.9075, Accuracy: 0.6875\n",
      "Batch number: 106, Training: Loss: 0.8287, Accuracy: 0.8125\n",
      "Batch number: 107, Training: Loss: 0.8599, Accuracy: 0.6250\n",
      "Batch number: 108, Training: Loss: 0.5928, Accuracy: 0.7500\n",
      "Batch number: 109, Training: Loss: 0.8251, Accuracy: 0.7500\n",
      "Batch number: 110, Training: Loss: 1.0107, Accuracy: 0.5000\n",
      "Batch number: 111, Training: Loss: 0.6554, Accuracy: 0.7500\n",
      "Batch number: 112, Training: Loss: 1.0698, Accuracy: 0.7500\n",
      "Batch number: 113, Training: Loss: 0.8817, Accuracy: 0.7500\n",
      "Batch number: 114, Training: Loss: 0.8641, Accuracy: 0.6250\n",
      "Batch number: 115, Training: Loss: 0.7407, Accuracy: 0.7500\n",
      "Batch number: 116, Training: Loss: 0.8121, Accuracy: 0.6875\n",
      "Batch number: 117, Training: Loss: 1.3629, Accuracy: 0.4375\n",
      "Batch number: 118, Training: Loss: 0.7837, Accuracy: 0.5625\n",
      "Batch number: 119, Training: Loss: 0.6339, Accuracy: 0.6875\n",
      "Batch number: 120, Training: Loss: 0.9459, Accuracy: 0.5000\n",
      "Batch number: 121, Training: Loss: 1.2215, Accuracy: 0.6875\n",
      "Batch number: 122, Training: Loss: 0.5324, Accuracy: 0.8125\n",
      "Batch number: 123, Training: Loss: 0.7169, Accuracy: 0.8125\n",
      "Batch number: 124, Training: Loss: 0.7732, Accuracy: 0.8125\n",
      "Batch number: 125, Training: Loss: 0.7656, Accuracy: 0.7500\n",
      "Batch number: 126, Training: Loss: 0.7231, Accuracy: 0.6875\n",
      "Batch number: 127, Training: Loss: 0.7299, Accuracy: 0.8125\n",
      "Batch number: 128, Training: Loss: 1.0151, Accuracy: 0.4375\n",
      "Batch number: 129, Training: Loss: 0.8242, Accuracy: 0.6250\n",
      "Batch number: 130, Training: Loss: 1.2140, Accuracy: 0.5625\n",
      "Batch number: 131, Training: Loss: 0.8328, Accuracy: 0.5625\n",
      "Batch number: 132, Training: Loss: 0.6483, Accuracy: 0.8125\n",
      "Batch number: 133, Training: Loss: 1.4273, Accuracy: 0.3750\n",
      "Batch number: 134, Training: Loss: 0.8160, Accuracy: 0.6250\n",
      "Batch number: 135, Training: Loss: 0.9606, Accuracy: 0.4375\n",
      "Batch number: 136, Training: Loss: 0.7580, Accuracy: 0.6250\n",
      "Batch number: 137, Training: Loss: 0.5486, Accuracy: 0.8125\n",
      "Batch number: 138, Training: Loss: 0.8361, Accuracy: 0.8125\n",
      "Batch number: 139, Training: Loss: 0.8701, Accuracy: 0.6875\n",
      "Batch number: 140, Training: Loss: 0.9515, Accuracy: 0.6250\n",
      "Batch number: 141, Training: Loss: 1.1530, Accuracy: 0.5000\n",
      "Batch number: 142, Training: Loss: 1.4649, Accuracy: 0.5000\n",
      "Batch number: 143, Training: Loss: 1.2987, Accuracy: 0.5000\n",
      "Batch number: 144, Training: Loss: 0.9936, Accuracy: 0.5000\n",
      "Batch number: 145, Training: Loss: 1.0935, Accuracy: 0.5625\n",
      "Batch number: 146, Training: Loss: 1.0656, Accuracy: 0.5625\n",
      "Batch number: 147, Training: Loss: 1.0750, Accuracy: 0.5625\n",
      "Batch number: 148, Training: Loss: 1.1192, Accuracy: 0.5000\n",
      "Batch number: 149, Training: Loss: 0.7836, Accuracy: 0.6875\n",
      "Batch number: 150, Training: Loss: 0.6294, Accuracy: 0.6250\n",
      "Batch number: 151, Training: Loss: 0.8392, Accuracy: 0.8125\n",
      "Batch number: 152, Training: Loss: 1.2256, Accuracy: 0.5000\n",
      "Batch number: 153, Training: Loss: 0.8813, Accuracy: 0.6250\n",
      "Batch number: 154, Training: Loss: 1.3797, Accuracy: 0.3333\n",
      "Validation Batch number: 000, Validation: Loss: 0.8419, Accuracy: 0.6875\n",
      "Validation Batch number: 001, Validation: Loss: 0.7151, Accuracy: 0.7500\n",
      "Validation Batch number: 002, Validation: Loss: 0.7670, Accuracy: 0.7500\n",
      "Validation Batch number: 003, Validation: Loss: 0.9956, Accuracy: 0.6875\n",
      "Validation Batch number: 004, Validation: Loss: 0.5210, Accuracy: 0.7500\n",
      "Validation Batch number: 005, Validation: Loss: 0.4551, Accuracy: 0.8125\n",
      "Validation Batch number: 006, Validation: Loss: 0.6654, Accuracy: 0.6250\n",
      "Validation Batch number: 007, Validation: Loss: 0.8741, Accuracy: 0.7500\n",
      "Validation Batch number: 008, Validation: Loss: 0.6209, Accuracy: 0.8125\n",
      "Validation Batch number: 009, Validation: Loss: 0.5921, Accuracy: 0.8125\n",
      "Validation Batch number: 010, Validation: Loss: 0.5626, Accuracy: 0.8750\n",
      "Validation Batch number: 011, Validation: Loss: 0.7269, Accuracy: 0.6875\n",
      "Validation Batch number: 012, Validation: Loss: 0.8803, Accuracy: 0.6250\n",
      "Validation Batch number: 013, Validation: Loss: 1.4579, Accuracy: 0.4375\n",
      "Validation Batch number: 014, Validation: Loss: 0.8163, Accuracy: 0.7500\n",
      "Validation Batch number: 015, Validation: Loss: 0.6507, Accuracy: 0.7500\n",
      "Validation Batch number: 016, Validation: Loss: 0.8560, Accuracy: 0.6875\n",
      "Validation Batch number: 017, Validation: Loss: 0.9258, Accuracy: 0.7500\n",
      "Validation Batch number: 018, Validation: Loss: 0.4467, Accuracy: 0.8125\n",
      "Validation Batch number: 019, Validation: Loss: 0.6814, Accuracy: 0.8125\n",
      "Validation Batch number: 020, Validation: Loss: 0.6159, Accuracy: 0.8125\n",
      "Validation Batch number: 021, Validation: Loss: 0.5704, Accuracy: 0.8125\n",
      "Validation Batch number: 022, Validation: Loss: 0.8404, Accuracy: 0.6875\n",
      "Validation Batch number: 023, Validation: Loss: 0.9474, Accuracy: 0.7500\n",
      "Validation Batch number: 024, Validation: Loss: 0.6624, Accuracy: 0.8000\n",
      "Epoch : 009, Training: Loss: 0.9420, Accuracy: 64.2075%, nttValidation : Loss : 0.7500, Accuracy: 73.7789%, Time: 1407.5089s\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "history = []\n",
    "best_loss = 100000.0\n",
    "best_epoch = None\n",
    "\n",
    "for epoch in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "        print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n",
    "        # Set to training mode\n",
    "        resnet18.train()\n",
    "        # Loss and Accuracy within the epoch\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        test_loss = 0.0\n",
    "        test_acc = 0.0\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # Clean existing gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass - compute outputs on input data using the model\n",
    "            outputs = resnet18(inputs)\n",
    "            # Compute loss\n",
    "            loss = loss_func(outputs, labels)\n",
    "            # Backpropagate the gradients\n",
    "            loss.backward()\n",
    "            # Update the parameters\n",
    "            optimizer.step()\n",
    "            # Compute the total loss for the batch and add it to train_loss\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            # Compute the accuracy\n",
    "            ret, predictions = torch.max(outputs.data, 1)\n",
    "            correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    "            # Convert correct_counts to float and then compute the mean\n",
    "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "            # Compute total accuracy in the whole batch and add to train_acc\n",
    "            train_acc += acc.item() * inputs.size(0)\n",
    "            print(\"Batch number: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}\".format(i, loss.item(), acc.item()))\n",
    "\n",
    "        # Validation - No gradient tracking needed\n",
    "with torch.no_grad():\n",
    "    # Set to evaluation mode\n",
    "    resnet18.eval()\n",
    "    # Validation loop\n",
    "    for j, (inputs, labels) in enumerate(test_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # Forward pass - compute outputs on input data using the model\n",
    "        outputs = resnet18(inputs)\n",
    "        # Compute loss\n",
    "        loss = loss_func(outputs, labels)\n",
    "        # Compute the total loss for the batch and add it to valid_loss\n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "        # Calculate validation accuracy\n",
    "        ret, predictions = torch.max(outputs.data, 1)\n",
    "        correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    "        # Convert correct_counts to float and then compute the mean\n",
    "        acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "        # Compute total accuracy in the whole batch and add to valid_acc\n",
    "        test_acc += acc.item() * inputs.size(0)\n",
    "        print(\"Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n",
    "# Find average training loss and training accuracy\n",
    "avg_train_loss = train_loss/train_data_size \n",
    "avg_train_acc = train_acc/float(train_data_size)\n",
    "# Find average training loss and training accuracy\n",
    "avg_valid_loss = test_loss/test_data_size \n",
    "avg_valid_acc = test_acc/float(test_data_size)\n",
    "history.append([avg_train_loss, avg_valid_loss, avg_train_acc, avg_valid_acc])\n",
    "epoch_end = time.time()\n",
    "print(\"Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, nttValidation : Loss : {:.4f}, Accuracy: {:.4f}%, Time: {:.4f}s\".format(epoch, avg_train_loss, avg_train_acc*100, avg_valid_loss, avg_valid_acc*100, epoch_end-epoch_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = np.array(history)\n",
    "plt.plot(history[:,0:2])\n",
    "plt.legend(['Tr Loss', 'Val Loss'])\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim(0,1)\n",
    "#plt.savefig(dataset+'_loss_curve.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history[:,2:4])\n",
    "plt.legend(['Tr Accuracy', 'Val Accuracy'])\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0,1)\n",
    "#plt.savefig(+'_accuracy_curve.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9afe26fdbc1503f8272de669f4735f1a61076a7cd2dc7567362436287fc50dfa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
